{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b3e518",
   "metadata": {},
   "source": [
    "# üéØ Glimpse3D - Master Pipeline\n",
    "\n",
    "## Complete 2D Image ‚Üí 3D Gaussian Splat Pipeline\n",
    "\n",
    "This notebook runs the **entire Glimpse3D pipeline** end-to-end:\n",
    "\n",
    "```\n",
    "üì∑ Input Image\n",
    "    ‚Üì\n",
    "üî∑ TripoSR (0.5s) ‚Üí Initial 3D Mesh ‚Üí Gaussian Points\n",
    "    ‚Üì\n",
    "üé® SyncDreamer (2min) ‚Üí 16 Consistent Multi-View Images\n",
    "    ‚Üì  \n",
    "‚ú® SDXL Lightning + ControlNet ‚Üí Enhanced Views\n",
    "    ‚Üì\n",
    "üîÆ gsplat Optimization ‚Üí Refined Gaussians\n",
    "    ‚Üì\n",
    "üîÑ MVCRM ‚Üí Multi-View Consistent Refinement\n",
    "    ‚Üì\n",
    "üèÜ Final 3D Gaussian Splat Output\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with **T4 GPU** (free tier) or **A100** (faster)\n",
    "- ~12GB VRAM peak usage\n",
    "- ~30 minutes total runtime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b9736",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start\n",
    "\n",
    "1. Run all cells in order (Runtime ‚Üí Run all)\n",
    "2. Upload your image when prompted\n",
    "3. Wait ~30 minutes for full pipeline\n",
    "4. Download final results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75e6c",
   "metadata": {},
   "source": [
    "# Stage 0: Environment Setup\n",
    "\n",
    "This stage:\n",
    "1. Checks GPU availability and VRAM\n",
    "2. Clones the Glimpse3D repository for `ai_modules` utilities\n",
    "3. Installs all required dependencies\n",
    "4. Creates the output directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42332bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"üñ•Ô∏è Running in Colab: {IN_COLAB}\")\n",
    "\n",
    "if not IN_COLAB:\n",
    "    print(\"‚ö†Ô∏è This notebook is designed for Google Colab!\")\n",
    "    print(\"   Some features may not work locally.\")\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüì¶ PyTorch: {torch.__version__}\")\n",
    "print(f\"üî• CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_VRAM = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"üéÆ GPU: {GPU_NAME}\")\n",
    "    print(f\"üíæ VRAM: {GPU_VRAM:.1f} GB\")\n",
    "    \n",
    "    # Set batch sizes based on GPU\n",
    "    if GPU_VRAM >= 40:  # A100\n",
    "        BATCH_VIEW_NUM = 8\n",
    "        MC_RESOLUTION = 384\n",
    "        NUM_SAMPLES = 200000\n",
    "    elif GPU_VRAM >= 15:  # T4/V100\n",
    "        BATCH_VIEW_NUM = 4\n",
    "        MC_RESOLUTION = 256\n",
    "        NUM_SAMPLES = 100000\n",
    "    else:  # Lower VRAM\n",
    "        BATCH_VIEW_NUM = 2\n",
    "        MC_RESOLUTION = 192\n",
    "        NUM_SAMPLES = 50000\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Settings: batch_view={BATCH_VIEW_NUM}, resolution={MC_RESOLUTION}, samples={NUM_SAMPLES}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No GPU available! Enable GPU in Runtime ‚Üí Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8127aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Glimpse3D repository (for ai_modules utilities)\n",
    "GLIMPSE3D_REPO = \"/content/Glimpse-3D\"\n",
    "\n",
    "if not os.path.exists(GLIMPSE3D_REPO):\n",
    "    print(\"üì• Cloning Glimpse3D repository...\")\n",
    "    !git clone https://github.com/varunaditya27/Glimpse3D.git {GLIMPSE3D_REPO}\n",
    "else:\n",
    "    print(f\"‚úÖ Glimpse3D repo already exists at {GLIMPSE3D_REPO}\")\n",
    "\n",
    "# Add to Python path for ai_modules\n",
    "sys.path.insert(0, GLIMPSE3D_REPO)\n",
    "print(f\"‚úÖ Added {GLIMPSE3D_REPO} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "# Install all dependencies with PINNED VERSIONS for production stability\n",
    "print(\"üì¶ Installing dependencies (this takes ~5 minutes)...\")\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: Ensure PyTorch + CUDA version match\n",
    "# Colab default torch may conflict - reinstall with exact versions\n",
    "!pip uninstall torch torchvision torchaudio -y --quiet 2>/dev/null || true\n",
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118 --quiet\n",
    "\n",
    "# Core packages with pinned versions\n",
    "!pip install transformers==4.40.0 diffusers==0.27.2 accelerate==0.28.0 huggingface_hub safetensors==0.4.2 --quiet\n",
    "!pip install omegaconf==2.3.0 einops==0.7.0 pytorch-lightning==1.9.5 kornia==0.7.0 --quiet\n",
    "\n",
    "# TripoSR dependencies\n",
    "!pip install trimesh==4.2.0 rembg[gpu]==2.0.55 xatlas plyfile>=0.9.1 --quiet\n",
    "!pip install git+https://github.com/tatsy/torchmcubes.git --quiet\n",
    "\n",
    "# gsplat - must match torch 2.0.1+cu118\n",
    "!pip install gsplat==1.2.0 --quiet\n",
    "\n",
    "# SyncDreamer dependencies - PIN CLIP to known working commit\n",
    "!pip install git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33 --quiet\n",
    "!pip install taming-transformers-rom1504==0.0.6 --quiet\n",
    "\n",
    "# Image processing\n",
    "!pip install opencv-python-headless>=4.8.0 scikit-image>=0.21.0 imageio>=2.31.0 --quiet\n",
    "\n",
    "# Depth estimation\n",
    "!pip install timm --quiet\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed with pinned versions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f719d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "WORK_DIR = Path(\"/content/glimpse3d_pipeline\")\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DIRS = {\n",
    "    'input': WORK_DIR / 'input',\n",
    "    'triposr': WORK_DIR / 'stage1_triposr',\n",
    "    'syncdreamer': WORK_DIR / 'stage2_syncdreamer',\n",
    "    'enhanced': WORK_DIR / 'stage3_enhanced',\n",
    "    'gsplat': WORK_DIR / 'stage4_gsplat',\n",
    "    'mvcrm': WORK_DIR / 'stage5_mvcrm',\n",
    "    'output': WORK_DIR / 'final_output',\n",
    "}\n",
    "\n",
    "for name, path in DIRS.items():\n",
    "    path.mkdir(exist_ok=True)\n",
    "    print(f\"üìÅ {name}: {path}\")\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Aggressively clear GPU memory between stages.\"\"\"\n",
    "    # Multiple gc passes for thorough cleanup\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"üßπ GPU memory cleared. Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def safe_del(obj_name, globals_dict):\n",
    "    \"\"\"Safely delete an object if it exists.\"\"\"\n",
    "    if obj_name in globals_dict and globals_dict[obj_name] is not None:\n",
    "        del globals_dict[obj_name]\n",
    "        \n",
    "print(\"\\n‚úÖ Directory structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a932c3f",
   "metadata": {},
   "source": [
    "# Stage 1: Upload Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"üì§ Upload your image (JPG/PNG):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Save uploaded file\n",
    "INPUT_FILENAME = list(uploaded.keys())[0]\n",
    "INPUT_PATH = DIRS['input'] / INPUT_FILENAME\n",
    "\n",
    "with open(INPUT_PATH, 'wb') as f:\n",
    "    f.write(list(uploaded.values())[0])\n",
    "\n",
    "# Display\n",
    "input_image = Image.open(INPUT_PATH)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(input_image)\n",
    "plt.title(f\"Input: {INPUT_FILENAME} ({input_image.size[0]}x{input_image.size[1]})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to: {INPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec82a1a",
   "metadata": {},
   "source": [
    "# Stage 2: TripoSR - Initial 3D Reconstruction\n",
    "\n",
    "**Input:** Single image  \n",
    "**Output:** 3D mesh + Gaussian point cloud  \n",
    "**Time:** ~30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0772717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone TripoSR\n",
    "TRIPOSR_PATH = Path(\"/content/TripoSR\")\n",
    "\n",
    "if not TRIPOSR_PATH.exists():\n",
    "    print(\"üì• Cloning TripoSR...\")\n",
    "    !git clone https://github.com/VAST-AI-Research/TripoSR.git {TRIPOSR_PATH}\n",
    "\n",
    "sys.path.insert(0, str(TRIPOSR_PATH))\n",
    "os.chdir(TRIPOSR_PATH)\n",
    "print(f\"‚úÖ TripoSR ready at {TRIPOSR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tsr.system import TSR\n",
    "from tsr.utils import remove_background, resize_foreground\n",
    "import rembg\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî∑ STAGE 2: TripoSR 3D Reconstruction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüì• Loading TripoSR model...\")\n",
    "triposr_model = TSR.from_pretrained(\n",
    "    \"stabilityai/TripoSR\",\n",
    "    config_name=\"config.yaml\",\n",
    "    weight_name=\"model.ckpt\",\n",
    ")\n",
    "triposr_model.renderer.set_chunk_size(8192)\n",
    "triposr_model.to(device)\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# Preprocess image\n",
    "print(\"\\nüîß Preprocessing image...\")\n",
    "input_img = Image.open(INPUT_PATH)\n",
    "rembg_session = rembg.new_session()\n",
    "processed_img = remove_background(input_img, rembg_session)\n",
    "processed_img = resize_foreground(processed_img, 0.85)\n",
    "\n",
    "# Convert to RGB\n",
    "img_np = np.array(processed_img).astype(np.float32) / 255.0\n",
    "img_np = img_np[:, :, :3] * img_np[:, :, 3:4] + (1 - img_np[:, :, 3:4]) * 0.5\n",
    "processed_img = Image.fromarray((img_np * 255.0).astype(np.uint8))\n",
    "processed_img.save(DIRS['triposr'] / \"processed_input.png\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\nüöÄ Running TripoSR...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    scene_codes = triposr_model([processed_img], device=device)\n",
    "    meshes = triposr_model.extract_mesh(scene_codes, has_vertex_color=True, resolution=MC_RESOLUTION)\n",
    "\n",
    "mesh = meshes[0]\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Mesh generated in {elapsed:.2f}s\")\n",
    "print(f\"   Vertices: {len(mesh.vertices):,}\")\n",
    "print(f\"   Faces: {len(mesh.faces):,}\")\n",
    "\n",
    "# Save mesh\n",
    "mesh.export(str(DIRS['triposr'] / \"mesh.obj\"))\n",
    "mesh.export(str(DIRS['triposr'] / \"mesh.glb\"))\n",
    "print(f\"\\nüìÅ Saved mesh to {DIRS['triposr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70122c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mesh to Gaussian PLY\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def mesh_to_gaussian_ply(mesh, output_path, num_samples=100000):\n",
    "    \"\"\"Convert mesh to Gaussian Splat format.\"\"\"\n",
    "    print(f\"\\nüîÑ Sampling {num_samples:,} points...\")\n",
    "    \n",
    "    points, face_indices = mesh.sample(num_samples, return_index=True)\n",
    "    \n",
    "    if mesh.visual.vertex_colors is not None:\n",
    "        face_vertices = mesh.faces[face_indices]\n",
    "        vertex_colors = mesh.visual.vertex_colors[:, :3] / 255.0\n",
    "        colors = vertex_colors[face_vertices].mean(axis=1)\n",
    "    else:\n",
    "        colors = np.ones((num_samples, 3)) * 0.5\n",
    "    \n",
    "    num_points = len(points)\n",
    "    xyz = points.astype(np.float32)\n",
    "    \n",
    "    C0 = 0.28209479177387814\n",
    "    features_dc = ((colors - 0.5) / C0).astype(np.float32)\n",
    "    features_rest = np.zeros((num_points, 45), dtype=np.float32)\n",
    "    opacities = np.ones((num_points, 1), dtype=np.float32) * 2.2\n",
    "    scales = np.ones((num_points, 3), dtype=np.float32) * (-4.6)\n",
    "    # ‚úÖ gsplat uses wxyz quaternion convention: rot_0=w, rot_1=x, rot_2=y, rot_3=z\n",
    "    rotations = np.zeros((num_points, 4), dtype=np.float32)\n",
    "    rotations[:, 0] = 1.0  # w=1 (identity rotation in wxyz format)\n",
    "    \n",
    "    dtype_full = [\n",
    "        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "        ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4'),\n",
    "    ]\n",
    "    for i in range(45):\n",
    "        dtype_full.append((f'f_rest_{i}', 'f4'))\n",
    "    dtype_full.extend([\n",
    "        ('opacity', 'f4'),\n",
    "        ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),\n",
    "        ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4'),\n",
    "    ])\n",
    "    \n",
    "    elements = np.zeros(num_points, dtype=dtype_full)\n",
    "    elements['x'] = xyz[:, 0]\n",
    "    elements['y'] = xyz[:, 1]\n",
    "    elements['z'] = xyz[:, 2]\n",
    "    elements['f_dc_0'] = features_dc[:, 0]\n",
    "    elements['f_dc_1'] = features_dc[:, 1]\n",
    "    elements['f_dc_2'] = features_dc[:, 2]\n",
    "    for i in range(45):\n",
    "        elements[f'f_rest_{i}'] = features_rest[:, i]\n",
    "    elements['opacity'] = opacities[:, 0]\n",
    "    elements['scale_0'] = scales[:, 0]\n",
    "    elements['scale_1'] = scales[:, 1]\n",
    "    elements['scale_2'] = scales[:, 2]\n",
    "    elements['rot_0'] = rotations[:, 0]\n",
    "    elements['rot_1'] = rotations[:, 1]\n",
    "    elements['rot_2'] = rotations[:, 2]\n",
    "    elements['rot_3'] = rotations[:, 3]\n",
    "    \n",
    "    el = PlyElement.describe(elements, 'vertex')\n",
    "    PlyData([el]).write(output_path)\n",
    "    print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "INITIAL_PLY_PATH = DIRS['triposr'] / \"initial_gaussian.ply\"\n",
    "mesh_to_gaussian_ply(mesh, str(INITIAL_PLY_PATH), num_samples=NUM_SAMPLES)\n",
    "\n",
    "# Cleanup TripoSR\n",
    "del triposr_model, mesh, scene_codes\n",
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fc9c3",
   "metadata": {},
   "source": [
    "# Stage 3: SyncDreamer - Multi-View Generation\n",
    "\n",
    "**Input:** Processed image  \n",
    "**Output:** 16 consistent multi-view images  \n",
    "**Time:** ~2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17879096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé® STAGE 3: SyncDreamer Multi-View Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clone SyncDreamer\n",
    "SYNCDREAMER_PATH = Path(\"/content/SyncDreamer\")\n",
    "\n",
    "if not SYNCDREAMER_PATH.exists():\n",
    "    print(\"üì• Cloning SyncDreamer...\")\n",
    "    !git clone https://github.com/liuyuan-pal/SyncDreamer.git {SYNCDREAMER_PATH}\n",
    "\n",
    "# Download checkpoints\n",
    "CKPT_DIR = SYNCDREAMER_PATH / \"ckpt\"\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "!apt -y install -qq aria2\n",
    "\n",
    "CHECKPOINTS = {\n",
    "    \"syncdreamer-pretrain.ckpt\": \"https://huggingface.co/camenduru/SyncDreamer/resolve/main/syncdreamer-pretrain.ckpt\",\n",
    "    \"ViT-L-14.pt\": \"https://huggingface.co/camenduru/SyncDreamer/resolve/main/ViT-L-14.pt\"\n",
    "}\n",
    "\n",
    "for fname, url in CHECKPOINTS.items():\n",
    "    fpath = CKPT_DIR / fname\n",
    "    if not fpath.exists():\n",
    "        print(f\"üì• Downloading {fname}...\")\n",
    "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d \"{CKPT_DIR}\" -o \"{fname}\"\n",
    "    else:\n",
    "        print(f\"‚úÖ {fname} exists\")\n",
    "\n",
    "sys.path.insert(0, str(SYNCDREAMER_PATH))\n",
    "os.chdir(SYNCDREAMER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "# Load SyncDreamer model\n",
    "print(\"\\nüì• Loading SyncDreamer model...\")\n",
    "\n",
    "config_path = SYNCDREAMER_PATH / \"configs\" / \"syncdreamer.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "# Instantiate model from config\n",
    "syncdreamer_model = instantiate_from_config(config.model)\n",
    "\n",
    "# Load pretrained weights\n",
    "ckpt_path = CKPT_DIR / \"syncdreamer-pretrain.ckpt\"\n",
    "print(f\"   Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"state_dict\"] if \"state_dict\" in checkpoint else checkpoint\n",
    "\n",
    "# Load with strict=False to handle any minor mismatches\n",
    "missing, unexpected = syncdreamer_model.load_state_dict(state_dict, strict=False)\n",
    "if missing:\n",
    "    print(f\"   ‚ö†Ô∏è Missing keys: {len(missing)}\")\n",
    "if unexpected:\n",
    "    print(f\"   ‚ö†Ô∏è Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "syncdreamer_model = syncdreamer_model.cuda().eval()\n",
    "\n",
    "# Verify model is ready\n",
    "print(f\"‚úÖ SyncDreamer loaded!\")\n",
    "print(f\"   Model type: {type(syncdreamer_model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.models.diffusion.sync_dreamer import SyncDDIMSampler\n",
    "from ldm.util import prepare_inputs  # CRITICAL: Use official data preparation\n",
    "\n",
    "# ‚úÖ FIXED: Camera configuration MUST match SyncDreamer training data\n",
    "# SyncDreamer generates 16 views at FIXED 30¬∞ elevation, azimuths spaced 22.5¬∞ apart\n",
    "ELEVATIONS = [30.0] * 16  # All 16 views at 30¬∞ elevation\n",
    "AZIMUTHS = [i * 22.5 for i in range(16)]  # 0¬∞, 22.5¬∞, 45¬∞, ..., 337.5¬∞\n",
    "RADIUS = 1.5\n",
    "\n",
    "# ‚úÖ FIXED: Use official prepare_inputs function for proper data preparation\n",
    "# This handles alpha channel, CLIP embedding, and proper normalization\n",
    "processed_path = DIRS['triposr'] / \"processed_input.png\"\n",
    "\n",
    "INPUT_ELEVATION = 30.0  # Assume front view at 30 degrees\n",
    "CROP_SIZE = 200         # Crop foreground to this size\n",
    "\n",
    "print(f\"üì∏ Preparing input: {processed_path}\")\n",
    "print(f\"   Input elevation: {INPUT_ELEVATION}¬∞\")\n",
    "print(f\"   Crop size: {CROP_SIZE}\")\n",
    "\n",
    "# Use official SyncDreamer data preparation\n",
    "data = prepare_inputs(str(processed_path), INPUT_ELEVATION, CROP_SIZE)\n",
    "\n",
    "# Move to GPU and add batch dimension\n",
    "for k, v in data.items():\n",
    "    data[k] = v.unsqueeze(0).cuda()\n",
    "    print(f\"   {k}: {data[k].shape}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Input prepared using official prepare_inputs()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2618cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SyncDreamer inference\n",
    "print(\"\\nüöÄ Running SyncDreamer (this takes ~2-3 minutes)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Settings\n",
    "SAMPLE_STEPS = 50\n",
    "CFG_SCALE = 2.0\n",
    "\n",
    "sampler = SyncDDIMSampler(syncdreamer_model, SAMPLE_STEPS)\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # ‚úÖ FIXED: Data already prepared correctly by prepare_inputs()\n",
    "        # Run synchronized multi-view generation\n",
    "        x_sample = syncdreamer_model.sample(\n",
    "            sampler, \n",
    "            data, \n",
    "            CFG_SCALE, \n",
    "            BATCH_VIEW_NUM\n",
    "        )\n",
    "        # x_sample shape: (B, N, C, H, W) where N=16 views\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"‚ö†Ô∏è OOM Error! Reducing batch size and retrying...\")\n",
    "        clear_gpu()\n",
    "        BATCH_VIEW_NUM = max(1, BATCH_VIEW_NUM // 2)\n",
    "        print(f\"   New BATCH_VIEW_NUM: {BATCH_VIEW_NUM}\")\n",
    "        \n",
    "        sampler = SyncDDIMSampler(syncdreamer_model, SAMPLE_STEPS)\n",
    "        with torch.no_grad():\n",
    "            x_sample = syncdreamer_model.sample(sampler, data, CFG_SCALE, BATCH_VIEW_NUM)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ SyncDreamer completed in {elapsed/60:.1f} minutes\")\n",
    "print(f\"   Output shape: {x_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d504fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save multi-view images\n",
    "print(\"\\nüíæ Saving multi-view images...\")\n",
    "\n",
    "# Convert samples to images: [-1,1] -> [0,1]\n",
    "samples = (x_sample.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "syncdreamer_views = []\n",
    "\n",
    "B, N, C, H, W = samples.shape\n",
    "print(f\"   Processing {N} views at {H}x{W}\")\n",
    "\n",
    "for i in range(N):\n",
    "    img_tensor = samples[0, i]  # (C, H, W)\n",
    "    img_np = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    img_pil = Image.fromarray(img_np)\n",
    "    \n",
    "    # Save individual view\n",
    "    elev = int(ELEVATIONS[i])\n",
    "    azim = int(AZIMUTHS[i])\n",
    "    save_path = DIRS['syncdreamer'] / f\"view_{i:02d}_e{elev}_a{azim}.png\"\n",
    "    img_pil.save(save_path)\n",
    "    syncdreamer_views.append(img_pil)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(syncdreamer_views)} views to {DIRS['syncdreamer']}\")\n",
    "\n",
    "# Display grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(syncdreamer_views[i])\n",
    "    ax.set_title(f\"E={int(ELEVATIONS[i])}¬∞ A={int(AZIMUTHS[i])}¬∞\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"SyncDreamer: 16 Multi-View Images\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS['syncdreamer'] / \"grid.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Cleanup SyncDreamer to free VRAM\n",
    "del syncdreamer_model, sampler, x_sample, samples\n",
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456650c8",
   "metadata": {},
   "source": [
    "# Stage 4: SDXL Enhancement (Optional)\n",
    "\n",
    "**Input:** Multi-view images  \n",
    "**Output:** Enhanced multi-view images  \n",
    "**Time:** ~1 minute per image\n",
    "\n",
    "Skip this stage if you want faster results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_ENHANCEMENT = False  # Set to True to skip this stage\n",
    "\n",
    "if not SKIP_ENHANCEMENT:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ú® STAGE 4: SDXL Lightning Enhancement\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from diffusers import StableDiffusionXLImg2ImgPipeline, AutoencoderKL, UNet2DConditionModel, EulerDiscreteScheduler\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    from safetensors.torch import load_file\n",
    "    \n",
    "    # ‚úÖ FIXED: Load SDXL Lightning (UNet checkpoint method - per official ByteDance docs)\n",
    "    print(\"\\nüì• Loading SDXL Lightning...\")\n",
    "    print(\"   Using UNet checkpoint method (recommended by ByteDance)\")\n",
    "    \n",
    "    base_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    repo = \"ByteDance/SDXL-Lightning\"\n",
    "    \n",
    "    try:\n",
    "        # Load VAE with fp16 fix\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            \"madebyollin/sdxl-vae-fp16-fix\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # Load UNet from base model config, then apply Lightning weights\n",
    "        unet = UNet2DConditionModel.from_config(base_model, subfolder=\"unet\")\n",
    "        unet = unet.to(dtype=torch.float16)\n",
    "        \n",
    "        # Download and load Lightning UNet weights (4-step version)\n",
    "        print(\"   Downloading Lightning UNet weights...\")\n",
    "        unet_path = hf_hub_download(repo, \"sdxl_lightning_4step_unet.safetensors\")\n",
    "        unet.load_state_dict(load_file(unet_path, device=\"cuda\"), strict=True)\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "            base_model,\n",
    "            unet=unet,\n",
    "            vae=vae,\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "        )\n",
    "        \n",
    "        # ‚úÖ CRITICAL: Use correct scheduler for Lightning\n",
    "        # Lightning was trained with \"trailing\" timestep spacing\n",
    "        pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
    "            pipe.scheduler.config, \n",
    "            timestep_spacing=\"trailing\"\n",
    "        )\n",
    "        \n",
    "        pipe = pipe.to(\"cuda\")\n",
    "        print(\"‚úÖ SDXL Lightning loaded!\")\n",
    "        print(\"   ‚ö†Ô∏è Remember: guidance_scale MUST be 0 for Lightning\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load SDXL Lightning: {e}\")\n",
    "        print(\"   Falling back to skip enhancement\")\n",
    "        SKIP_ENHANCEMENT = True\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping enhancement stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_ENHANCEMENT:\n",
    "    # Enhance select views (not all 16 to save time)\n",
    "    VIEWS_TO_ENHANCE = [0, 2, 4, 6, 8, 10, 12, 14]  # Every other view\n",
    "    \n",
    "    print(f\"\\nüöÄ Enhancing {len(VIEWS_TO_ENHANCE)} views...\")\n",
    "    \n",
    "    enhanced_views = syncdreamer_views.copy()  # Start with original\n",
    "    \n",
    "    prompt = \"highly detailed 3D render, professional studio lighting, sharp textures, photorealistic, 8k quality\"\n",
    "    negative_prompt = \"blurry, low quality, artifacts, noise, watermark, text\"\n",
    "    \n",
    "    for i, view_idx in enumerate(VIEWS_TO_ENHANCE):\n",
    "        print(f\"  Enhancing view {view_idx} ({i+1}/{len(VIEWS_TO_ENHANCE)})...\")\n",
    "        \n",
    "        # Resize input for SDXL (works best at 512-1024)\n",
    "        input_img = syncdreamer_views[view_idx].resize((512, 512), Image.LANCZOS)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=input_img,\n",
    "                strength=0.35,  # Lower = preserve more original structure\n",
    "                num_inference_steps=4,  # Lightning uses 4 steps\n",
    "                guidance_scale=0,  # Lightning uses CFG=0\n",
    "            ).images[0]\n",
    "        \n",
    "        # Resize back to match SyncDreamer output size\n",
    "        result_resized = result.resize((256, 256), Image.LANCZOS)\n",
    "        enhanced_views[view_idx] = result_resized\n",
    "        result.save(DIRS['enhanced'] / f\"enhanced_{view_idx:02d}.png\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Enhanced views saved to {DIRS['enhanced']}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del pipe, vae, unet\n",
    "    clear_gpu()\n",
    "else:\n",
    "    enhanced_views = syncdreamer_views\n",
    "    print(\"Using original SyncDreamer views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cacdbd",
   "metadata": {},
   "source": [
    "# Stage 5: gsplat Optimization\n",
    "\n",
    "**Input:** Initial Gaussian PLY + Multi-view images  \n",
    "**Output:** Optimized Gaussian Splats  \n",
    "**Time:** ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÆ STAGE 5: gsplat Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import torch.nn as nn\n",
    "from gsplat import rasterization\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Image size for rendering (matches SyncDreamer output)\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "# Load initial Gaussians from PLY\n",
    "def load_gaussian_ply(path):\n",
    "    \"\"\"Load Gaussian parameters from PLY file.\"\"\"\n",
    "    plydata = PlyData.read(path)\n",
    "    vertex = plydata['vertex']\n",
    "    \n",
    "    xyz = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=-1)\n",
    "    f_dc = np.stack([vertex['f_dc_0'], vertex['f_dc_1'], vertex['f_dc_2']], axis=-1)\n",
    "    \n",
    "    # Load f_rest if present\n",
    "    f_rest_names = [f'f_rest_{i}' for i in range(45)]\n",
    "    available_f_rest = [name for name in f_rest_names if name in vertex.data.dtype.names]\n",
    "    if available_f_rest:\n",
    "        f_rest = np.stack([vertex[name] for name in available_f_rest], axis=-1)\n",
    "    else:\n",
    "        f_rest = np.zeros((len(xyz), 45), dtype=np.float32)\n",
    "    \n",
    "    # ‚úÖ FIXED: Ensure opacity is 1D (N,) - gsplat requires this shape\n",
    "    opacity = np.asarray(vertex['opacity'], dtype=np.float32).flatten()\n",
    "    scales = np.stack([vertex['scale_0'], vertex['scale_1'], vertex['scale_2']], axis=-1)\n",
    "    rotations = np.stack([vertex['rot_0'], vertex['rot_1'], vertex['rot_2'], vertex['rot_3']], axis=-1)\n",
    "    \n",
    "    return {\n",
    "        'xyz': torch.tensor(xyz, dtype=torch.float32),\n",
    "        'f_dc': torch.tensor(f_dc, dtype=torch.float32),\n",
    "        'f_rest': torch.tensor(f_rest, dtype=torch.float32),\n",
    "        'opacity': torch.tensor(opacity, dtype=torch.float32),  # Shape: (N,)\n",
    "        'scales': torch.tensor(scales, dtype=torch.float32),\n",
    "        'rotations': torch.tensor(rotations, dtype=torch.float32),\n",
    "    }\n",
    "\n",
    "gaussians = load_gaussian_ply(str(INITIAL_PLY_PATH))\n",
    "print(f\"‚úÖ Loaded {len(gaussians['xyz']):,} Gaussians\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianModel(nn.Module):\n",
    "    def __init__(self, gaussians):\n",
    "        super().__init__()\n",
    "        self.xyz = nn.Parameter(gaussians['xyz'].clone())\n",
    "        self.f_dc = nn.Parameter(gaussians['f_dc'].clone())\n",
    "        self.f_rest = nn.Parameter(gaussians['f_rest'].clone())\n",
    "        # ‚úÖ FIXED: Ensure opacity is 1D (N,) - gsplat requires this shape\n",
    "        opacity_tensor = gaussians['opacity'].clone()\n",
    "        if opacity_tensor.dim() > 1:\n",
    "            opacity_tensor = opacity_tensor.squeeze(-1)\n",
    "        self.opacity_raw = nn.Parameter(opacity_tensor)\n",
    "        self.scales_raw = nn.Parameter(gaussians['scales'].clone())\n",
    "        self.rotations = nn.Parameter(gaussians['rotations'].clone())\n",
    "        \n",
    "    @property\n",
    "    def opacity(self):\n",
    "        # ‚úÖ FIXED: Returns (N,) shape - required by gsplat.rasterization()\n",
    "        return torch.sigmoid(self.opacity_raw)\n",
    "    \n",
    "    @property\n",
    "    def scales(self):\n",
    "        return torch.exp(self.scales_raw)\n",
    "    \n",
    "    def get_colors(self):\n",
    "        C0 = 0.28209479177387814\n",
    "        return 0.5 + C0 * self.f_dc\n",
    "    \n",
    "    def forward(self):\n",
    "        return {\n",
    "            'xyz': self.xyz,\n",
    "            'colors': self.get_colors(),\n",
    "            'opacity': self.opacity,\n",
    "            'scales': self.scales,\n",
    "            'rotations': self.rotations / (self.rotations.norm(dim=-1, keepdim=True) + 1e-8),\n",
    "        }\n",
    "\n",
    "model = GaussianModel(gaussians).to(device)\n",
    "print(f\"‚úÖ Model: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera system matching SyncDreamer conventions\n",
    "# SyncDreamer uses: Y-up, camera looks at origin, radius ~1.5\n",
    "\n",
    "def create_camera_pose(elevation_deg, azimuth_deg, radius=1.5):\n",
    "    \"\"\"Create world-to-camera matrix for given elevation and azimuth.\"\"\"\n",
    "    elev = math.radians(elevation_deg)\n",
    "    azim = math.radians(azimuth_deg)\n",
    "    \n",
    "    # Camera position in spherical coordinates (Y-up convention)\n",
    "    x = radius * math.cos(elev) * math.sin(azim)\n",
    "    y = radius * math.sin(elev)\n",
    "    z = radius * math.cos(elev) * math.cos(azim)\n",
    "    \n",
    "    cam_pos = np.array([x, y, z])\n",
    "    look_at = np.array([0, 0, 0])\n",
    "    up = np.array([0, 1, 0])  # Y-up\n",
    "    \n",
    "    # Construct camera basis\n",
    "    forward = look_at - cam_pos\n",
    "    forward = forward / (np.linalg.norm(forward) + 1e-8)\n",
    "    right = np.cross(forward, up)\n",
    "    right = right / (np.linalg.norm(right) + 1e-8)\n",
    "    up_new = np.cross(right, forward)\n",
    "    \n",
    "    # World-to-camera transformation\n",
    "    # R rotates world to camera, t translates\n",
    "    w2c = np.eye(4, dtype=np.float32)\n",
    "    w2c[0, :3] = right\n",
    "    w2c[1, :3] = up_new\n",
    "    w2c[2, :3] = -forward  # Camera looks along -Z\n",
    "    w2c[:3, 3] = -w2c[:3, :3] @ cam_pos\n",
    "    \n",
    "    return w2c\n",
    "\n",
    "def get_intrinsics(fov_deg=49.1, image_size=256):\n",
    "    \"\"\"Get camera intrinsics matrix. FOV ~49.1 matches SyncDreamer.\"\"\"\n",
    "    fov_rad = math.radians(fov_deg)\n",
    "    focal = image_size / (2 * math.tan(fov_rad / 2))\n",
    "    \n",
    "    K = np.array([\n",
    "        [focal, 0, image_size / 2],\n",
    "        [0, focal, image_size / 2],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "    return K\n",
    "\n",
    "# Pre-compute all camera poses (using SyncDreamer camera parameters)\n",
    "camera_poses = [create_camera_pose(e, a, radius=RADIUS) for e, a in zip(ELEVATIONS, AZIMUTHS)]\n",
    "intrinsics = get_intrinsics(fov_deg=49.1, image_size=IMAGE_SIZE)\n",
    "\n",
    "print(f\"‚úÖ Created {len(camera_poses)} camera poses\")\n",
    "print(f\"   Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"   Intrinsics: focal={intrinsics[0,0]:.1f}, center=({intrinsics[0,2]:.0f}, {intrinsics[1,2]:.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a425de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_gaussians(model, w2c, K, image_size):\n",
    "    \"\"\"Render Gaussian splats from a camera viewpoint.\"\"\"\n",
    "    params = model()\n",
    "    \n",
    "    viewmat = torch.tensor(w2c, dtype=torch.float32, device=device)\n",
    "    K_tensor = torch.tensor(K, dtype=torch.float32, device=device)\n",
    "    \n",
    "    try:\n",
    "        render_colors, render_alphas, info = rasterization(\n",
    "            means=params['xyz'],\n",
    "            quats=params['rotations'],\n",
    "            scales=params['scales'],\n",
    "            opacities=params['opacity'],\n",
    "            colors=params['colors'],\n",
    "            viewmats=viewmat.unsqueeze(0),\n",
    "            Ks=K_tensor.unsqueeze(0),\n",
    "            width=image_size,\n",
    "            height=image_size,\n",
    "            packed=False,\n",
    "            render_mode=\"RGB\",\n",
    "        )\n",
    "        return render_colors[0], render_alphas[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Render error: {e}\")\n",
    "        # Return empty image on error\n",
    "        return torch.zeros(image_size, image_size, 3, device=device), torch.zeros(image_size, image_size, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38273f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Prepare target images as tensors\n",
    "target_tensors = []\n",
    "for img in enhanced_views:\n",
    "    img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.LANCZOS)\n",
    "    img_tensor = torch.tensor(np.array(img_resized) / 255.0, dtype=torch.float32, device=device)\n",
    "    target_tensors.append(img_tensor)\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(target_tensors)} target images at {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "\n",
    "# Optimizer with appropriate learning rates\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.xyz, 'lr': 1e-4, 'name': 'xyz'},\n",
    "    {'params': model.f_dc, 'lr': 2.5e-3, 'name': 'f_dc'},\n",
    "    {'params': model.f_rest, 'lr': 2.5e-3 / 20, 'name': 'f_rest'},\n",
    "    {'params': model.opacity_raw, 'lr': 0.05, 'name': 'opacity'},\n",
    "    {'params': model.scales_raw, 'lr': 5e-3, 'name': 'scales'},\n",
    "    {'params': model.rotations, 'lr': 1e-3, 'name': 'rotations'},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "\n",
    "# Training loop\n",
    "NUM_ITERATIONS = 1000\n",
    "losses = []\n",
    "\n",
    "print(f\"\\nüöÄ Starting optimization for {NUM_ITERATIONS} iterations...\")\n",
    "pbar = tqdm(range(NUM_ITERATIONS))\n",
    "\n",
    "for iteration in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample random view\n",
    "    view_idx = np.random.randint(0, 16)\n",
    "    w2c = camera_poses[view_idx]\n",
    "    target = target_tensors[view_idx]\n",
    "    \n",
    "    # Render\n",
    "    rendered, alpha = render_gaussians(model, w2c, intrinsics, IMAGE_SIZE)\n",
    "    \n",
    "    # L1 + SSIM loss (L1 is more stable)\n",
    "    l1_loss = F.l1_loss(rendered, target)\n",
    "    loss = l1_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'view': view_idx})\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization complete!\")\n",
    "print(f\"   Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"   Final loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('gsplat Optimization Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig(DIRS['gsplat'] / \"loss_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized model\n",
    "def save_gaussian_ply(model, output_path):\n",
    "    with torch.no_grad():\n",
    "        params = model()\n",
    "        xyz = params['xyz'].cpu().numpy()\n",
    "        colors = model.f_dc.cpu().numpy()\n",
    "        f_rest = model.f_rest.cpu().numpy()\n",
    "        opacity = model.opacity_raw.cpu().numpy()\n",
    "        scales = model.scales_raw.cpu().numpy()\n",
    "        rotations = params['rotations'].cpu().numpy()\n",
    "        \n",
    "    num_points = len(xyz)\n",
    "    dtype_full = [('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "                  ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4')]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        dtype_full.append((f'f_rest_{i}', 'f4'))\n",
    "    dtype_full.extend([('opacity', 'f4'),\n",
    "                       ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),\n",
    "                       ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4')])\n",
    "    \n",
    "    elements = np.zeros(num_points, dtype=dtype_full)\n",
    "    elements['x'] = xyz[:, 0]\n",
    "    elements['y'] = xyz[:, 1]\n",
    "    elements['z'] = xyz[:, 2]\n",
    "    elements['f_dc_0'] = colors[:, 0]\n",
    "    elements['f_dc_1'] = colors[:, 1]\n",
    "    elements['f_dc_2'] = colors[:, 2]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        elements[f'f_rest_{i}'] = f_rest[:, i]\n",
    "    elements['opacity'] = opacity\n",
    "    elements['scale_0'] = scales[:, 0]\n",
    "    elements['scale_1'] = scales[:, 1]\n",
    "    elements['scale_2'] = scales[:, 2]\n",
    "    elements['rot_0'] = rotations[:, 0]\n",
    "    elements['rot_1'] = rotations[:, 1]\n",
    "    elements['rot_2'] = rotations[:, 2]\n",
    "    elements['rot_3'] = rotations[:, 3]\n",
    "    \n",
    "    el = PlyElement.describe(elements, 'vertex')\n",
    "    PlyData([el]).write(output_path)\n",
    "\n",
    "OPTIMIZED_PLY_PATH = DIRS['gsplat'] / \"optimized_gaussian.ply\"\n",
    "save_gaussian_ply(model, str(OPTIMIZED_PLY_PATH))\n",
    "print(f\"‚úÖ Saved optimized Gaussians: {OPTIMIZED_PLY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc775e",
   "metadata": {},
   "source": [
    "# Stage 6: Generate Final Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ FINAL OUTPUT GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import imageio\n",
    "\n",
    "# Generate 360¬∞ video at higher resolution\n",
    "VIDEO_SIZE = 512\n",
    "video_intrinsics = get_intrinsics(fov_deg=49.1, image_size=VIDEO_SIZE)\n",
    "\n",
    "print(\"\\nüé¨ Rendering 360¬∞ turntable video...\")\n",
    "video_frames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for azim in tqdm(np.linspace(0, 360, 120, endpoint=False)):\n",
    "        w2c = create_camera_pose(20.0, azim, radius=RADIUS)  # Slightly lower angle for nicer view\n",
    "        rgb, _ = render_gaussians(model, w2c, video_intrinsics, VIDEO_SIZE)\n",
    "        frame = (rgb.cpu().numpy().clip(0, 1) * 255).astype(np.uint8)\n",
    "        video_frames.append(frame)\n",
    "\n",
    "video_path = DIRS['output'] / \"glimpse3d_360.mp4\"\n",
    "imageio.mimsave(str(video_path), video_frames, fps=30)\n",
    "print(f\"‚úÖ Video saved: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdffe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy final files\n",
    "import shutil\n",
    "\n",
    "# Copy optimized PLY\n",
    "final_ply = DIRS['output'] / \"final_gaussian.ply\"\n",
    "shutil.copy(OPTIMIZED_PLY_PATH, final_ply)\n",
    "\n",
    "# Copy mesh\n",
    "shutil.copy(DIRS['triposr'] / \"mesh.glb\", DIRS['output'] / \"initial_mesh.glb\")\n",
    "shutil.copy(DIRS['triposr'] / \"mesh.obj\", DIRS['output'] / \"initial_mesh.obj\")\n",
    "\n",
    "# Copy best views\n",
    "for i in [0, 4, 8, 12]:\n",
    "    shutil.copy(\n",
    "        DIRS['syncdreamer'] / f\"view_{i:02d}_e{int(ELEVATIONS[i])}_a{int(AZIMUTHS[i])}.png\",\n",
    "        DIRS['output'] / f\"view_{i:02d}.png\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüìÅ Final output files:\")\n",
    "for f in sorted(DIRS['output'].iterdir()):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e210fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display video\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(video_path, 'rb').read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "HTML(f'''\n",
    "<h3>üèÜ Glimpse3D Result</h3>\n",
    "<video width=\"600\" controls autoplay loop>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddae42",
   "metadata": {},
   "source": [
    "# üì• Download All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Create final ZIP\n",
    "output_zip = str(WORK_DIR / \"glimpse3d_complete_output\")\n",
    "shutil.make_archive(output_zip, 'zip', DIRS['output'])\n",
    "\n",
    "print(\"üì• Downloading Glimpse3D results...\")\n",
    "files.download(f\"{output_zip}.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ GLIMPSE3D PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDownloaded: glimpse3d_complete_output.zip\")\n",
    "print(\"\\nContents:\")\n",
    "print(\"  - final_gaussian.ply   : Optimized Gaussian Splats\")\n",
    "print(\"  - initial_mesh.glb/obj : TripoSR mesh\")\n",
    "print(\"  - glimpse3d_360.mp4    : 360¬∞ turntable video\")\n",
    "print(\"  - view_*.png           : Multi-view images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ace8d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Pipeline Complete!\n",
    "\n",
    "You now have:\n",
    "1. **final_gaussian.ply** - View in any Gaussian Splat viewer\n",
    "2. **initial_mesh.glb** - View in 3D viewers like Blender, online GLB viewers\n",
    "3. **glimpse3d_360.mp4** - Share as video\n",
    "\n",
    "### Recommended Viewers\n",
    "- **Gaussian Splats**: [SuperSplat](https://playcanvas.com/supersplat/editor), [Luma AI Viewer](https://lumalabs.ai/)\n",
    "- **GLB Mesh**: [glTF Viewer](https://gltf-viewer.donmccurdy.com/), Blender\n",
    "\n",
    "### Tips for Better Results\n",
    "1. Use high-quality input images with clean backgrounds\n",
    "2. Objects should be centered and fill ~80% of the frame\n",
    "3. Avoid reflective or transparent surfaces\n",
    "4. Run more gsplat iterations (2000+) for higher quality"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
