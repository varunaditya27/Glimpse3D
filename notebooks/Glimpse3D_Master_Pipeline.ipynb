{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b3e518",
   "metadata": {},
   "source": [
    "# üéØ Glimpse3D - Master Pipeline\n",
    "\n",
    "## Complete 2D Image ‚Üí 3D Gaussian Splat Pipeline\n",
    "\n",
    "This notebook runs the **entire Glimpse3D pipeline** end-to-end:\n",
    "\n",
    "```\n",
    "üì∑ Input Image\n",
    "    ‚Üì\n",
    "üî∑ TripoSR (0.5s) ‚Üí Initial 3D Mesh ‚Üí Gaussian Points\n",
    "    ‚Üì\n",
    "üé® SyncDreamer (2min) ‚Üí 16 Consistent Multi-View Images\n",
    "    ‚Üì  \n",
    "‚ú® SDXL Lightning + ControlNet ‚Üí Enhanced Views\n",
    "    ‚Üì\n",
    "üîÆ gsplat Optimization ‚Üí Refined Gaussians\n",
    "    ‚Üì\n",
    "üîÑ MVCRM ‚Üí Multi-View Consistent Refinement\n",
    "    ‚Üì\n",
    "üèÜ Final 3D Gaussian Splat Output\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with **T4 GPU** (free tier) or **A100** (faster)\n",
    "- ~12GB VRAM peak usage\n",
    "- ~30 minutes total runtime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b9736",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start\n",
    "\n",
    "1. Run all cells in order (Runtime ‚Üí Run all)\n",
    "2. Upload your image when prompted\n",
    "3. Wait ~30 minutes for full pipeline\n",
    "4. Download final results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75e6c",
   "metadata": {},
   "source": [
    "# Stage 0: Environment Setup\n",
    "\n",
    "This stage:\n",
    "1. Checks GPU availability and VRAM\n",
    "2. Clones the Glimpse3D repository for `ai_modules` utilities\n",
    "3. Installs all required dependencies\n",
    "4. Creates the output directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42332bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"üñ•Ô∏è Running in Colab: {IN_COLAB}\")\n",
    "\n",
    "if not IN_COLAB:\n",
    "    print(\"‚ö†Ô∏è This notebook is designed for Google Colab!\")\n",
    "    print(\"   Some features may not work locally.\")\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüì¶ PyTorch: {torch.__version__}\")\n",
    "print(f\"üî• CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_VRAM = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"üéÆ GPU: {GPU_NAME}\")\n",
    "    print(f\"üíæ VRAM: {GPU_VRAM:.1f} GB\")\n",
    "    \n",
    "    # Set batch sizes based on GPU\n",
    "    if GPU_VRAM >= 40:  # A100\n",
    "        BATCH_VIEW_NUM = 8\n",
    "        MC_RESOLUTION = 384\n",
    "        NUM_SAMPLES = 200000\n",
    "    elif GPU_VRAM >= 15:  # T4/V100\n",
    "        BATCH_VIEW_NUM = 4\n",
    "        MC_RESOLUTION = 256\n",
    "        NUM_SAMPLES = 100000\n",
    "    else:  # Lower VRAM\n",
    "        BATCH_VIEW_NUM = 2\n",
    "        MC_RESOLUTION = 192\n",
    "        NUM_SAMPLES = 50000\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Settings: batch_view={BATCH_VIEW_NUM}, resolution={MC_RESOLUTION}, samples={NUM_SAMPLES}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No GPU available! Enable GPU in Runtime ‚Üí Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8127aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Glimpse3D repository (for ai_modules utilities)\n",
    "GLIMPSE3D_REPO = \"/content/Glimpse-3D\"\n",
    "\n",
    "if not os.path.exists(GLIMPSE3D_REPO):\n",
    "    print(\"üì• Cloning Glimpse3D repository...\")\n",
    "    !git clone https://github.com/varunaditya27/Glimpse3D.git {GLIMPSE3D_REPO}\n",
    "else:\n",
    "    print(f\"‚úÖ Glimpse3D repo already exists at {GLIMPSE3D_REPO}\")\n",
    "\n",
    "# Add to Python path for ai_modules\n",
    "sys.path.insert(0, GLIMPSE3D_REPO)\n",
    "print(f\"‚úÖ Added {GLIMPSE3D_REPO} to Python path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d008c57",
   "metadata": {},
   "source": [
    "## üì¶ Installing Dependencies\n",
    "\n",
    "**Strategy:**\n",
    "1. Install compatible NumPy + scipy first (avoids version conflicts)\n",
    "2. Install ML frameworks (transformers, diffusers, etc.)\n",
    "3. Install 3D processing packages (trimesh, rembg, gsplat)\n",
    "4. Install model-specific packages (CLIP, etc.)\n",
    "\n",
    "**Expected warnings:** Pip may show version conflicts for opencv/tensorflow/gradio - these are **harmless** because our pipeline doesn't use those pre-installed Colab packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "# Install all dependencies - carefully pinned for Google Colab compatibility\n",
    "# Note: Colab (Jan 2026) has NumPy 2.x, PyTorch 2.9+, CUDA 12.6\n",
    "# We work WITH these defaults, not against them.\n",
    "\n",
    "print(\"üì¶ Installing dependencies (this takes ~5 minutes)...\")\n",
    "\n",
    "# ===============================================\n",
    "# STRATEGY: Pin NumPy + scipy first to ensure compatibility\n",
    "# Then install everything else WITHOUT allowing numpy downgrades\n",
    "# Ignore pip warnings about opencv/tensorflow/gradio - we don't use them!\n",
    "# ===============================================\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL FIX: Install compatible NumPy + scipy FIRST\n",
    "# NumPy 2.1.x is well-supported and stable\n",
    "# scipy 1.14+ has good NumPy 2.x support\n",
    "print(\"üìå Step 1: Installing NumPy + scipy with compatible versions...\")\n",
    "!pip install \"numpy>=2.1,<2.2\" \"scipy>=1.14\" --quiet\n",
    "\n",
    "# Verify installation succeeded\n",
    "import numpy as np\n",
    "import scipy\n",
    "print(f\"‚úÖ NumPy: {np.__version__}, scipy: {scipy.__version__}\")\n",
    "\n",
    "# Core ML packages - pin to stable versions that work with NumPy 2.x\n",
    "print(\"üìå Step 2: Installing ML frameworks...\")\n",
    "!pip install transformers>=4.44.0 diffusers>=0.30.0 accelerate huggingface_hub safetensors --quiet\n",
    "!pip install omegaconf einops pytorch-lightning>=2.0.0 kornia --quiet\n",
    "\n",
    "# TripoSR dependencies \n",
    "# CRITICAL: trimesh>=4.4.0 is required for NumPy 2.0 compatibility (ptp fix)\n",
    "print(\"üìå Step 3: Installing 3D processing packages...\")\n",
    "!pip install \"trimesh>=4.4.0\" rembg[gpu] xatlas plyfile --quiet\n",
    "\n",
    "# torchmcubes for marching cubes\n",
    "!pip install git+https://github.com/tatsy/torchmcubes.git --quiet\n",
    "\n",
    "# gsplat - builds JIT, compatible with Colab's PyTorch\n",
    "print(\"üìå Step 4: Installing gsplat...\")\n",
    "!pip install gsplat --quiet\n",
    "\n",
    "# SyncDreamer dependencies  \n",
    "# Pin CLIP to specific commit for stability\n",
    "print(\"üìå Step 5: Installing SyncDreamer dependencies...\")\n",
    "!pip install git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33 --quiet\n",
    "!pip install taming-transformers-rom1504 --quiet\n",
    "\n",
    "# Image processing - use Colab's scikit-image, just ensure imageio\n",
    "print(\"üìå Step 6: Installing image processing...\")\n",
    "!pip install imageio imageio-ffmpeg --quiet\n",
    "\n",
    "# Depth estimation (MiDaS)\n",
    "!pip install timm --quiet\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")\n",
    "print(\"‚ö†Ô∏è Pip warnings about opencv/tensorflow/gradio can be ignored - we don't use those packages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effa84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key dependencies are correctly installed\n",
    "print(\"üîç Verifying critical dependencies...\\n\")\n",
    "\n",
    "import numpy as np\n",
    "print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Verify scipy is compatible with numpy (this was causing the _center import error)\n",
    "import scipy\n",
    "print(f\"‚úÖ scipy: {scipy.__version__}\")\n",
    "# Quick test to ensure scipy.sparse works\n",
    "try:\n",
    "    import scipy.sparse\n",
    "    print(\"   scipy.sparse: ‚úÖ OK\")\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(f\"‚ùå scipy is NOT compatible with NumPy! Error: {e}\")\n",
    "\n",
    "import trimesh\n",
    "print(f\"‚úÖ trimesh: {trimesh.__version__}\")\n",
    "# Verify trimesh works with NumPy 2.x (ptp fix check)\n",
    "try:\n",
    "    test_mesh = trimesh.creation.box()\n",
    "    _ = test_mesh.bounds  # This uses np.ptp internally in old versions\n",
    "    print(\"   NumPy 2.x compatibility: ‚úÖ OK\")\n",
    "except AttributeError as e:\n",
    "    if 'ptp' in str(e):\n",
    "        raise RuntimeError(\"‚ùå trimesh is NOT compatible with NumPy 2.x! Please install trimesh>=4.4.0\")\n",
    "    raise\n",
    "\n",
    "import transformers\n",
    "print(f\"‚úÖ transformers: {transformers.__version__}\")\n",
    "\n",
    "import diffusers\n",
    "print(f\"‚úÖ diffusers: {diffusers.__version__}\")\n",
    "\n",
    "# Test rembg (background removal) - this depends on scipy working\n",
    "try:\n",
    "    import rembg\n",
    "    print(f\"‚úÖ rembg: installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå rembg import failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test plyfile\n",
    "try:\n",
    "    from plyfile import PlyData\n",
    "    print(f\"‚úÖ plyfile: installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå plyfile not found!\")\n",
    "\n",
    "print(\"\\nüéâ All critical dependencies verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f719d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "WORK_DIR = Path(\"/content/glimpse3d_pipeline\")\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DIRS = {\n",
    "    'input': WORK_DIR / 'input',\n",
    "    'triposr': WORK_DIR / 'stage1_triposr',\n",
    "    'syncdreamer': WORK_DIR / 'stage2_syncdreamer',\n",
    "    'enhanced': WORK_DIR / 'stage3_enhanced',\n",
    "    'gsplat': WORK_DIR / 'stage4_gsplat',\n",
    "    'mvcrm': WORK_DIR / 'stage5_mvcrm',\n",
    "    'output': WORK_DIR / 'final_output',\n",
    "}\n",
    "\n",
    "for name, path in DIRS.items():\n",
    "    path.mkdir(exist_ok=True)\n",
    "    print(f\"üìÅ {name}: {path}\")\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Aggressively clear GPU memory between stages.\"\"\"\n",
    "    # Multiple gc passes for thorough cleanup\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"üßπ GPU memory cleared. Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def safe_del(obj_name, globals_dict):\n",
    "    \"\"\"Safely delete an object if it exists.\"\"\"\n",
    "    if obj_name in globals_dict and globals_dict[obj_name] is not None:\n",
    "        del globals_dict[obj_name]\n",
    "        \n",
    "print(\"\\n‚úÖ Directory structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a932c3f",
   "metadata": {},
   "source": [
    "# Stage 1: Upload Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"üì§ Upload your image (JPG/PNG):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Save uploaded file\n",
    "INPUT_FILENAME = list(uploaded.keys())[0]\n",
    "INPUT_PATH = DIRS['input'] / INPUT_FILENAME\n",
    "\n",
    "with open(INPUT_PATH, 'wb') as f:\n",
    "    f.write(list(uploaded.values())[0])\n",
    "\n",
    "# Display\n",
    "input_image = Image.open(INPUT_PATH)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(input_image)\n",
    "plt.title(f\"Input: {INPUT_FILENAME} ({input_image.size[0]}x{input_image.size[1]})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to: {INPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec82a1a",
   "metadata": {},
   "source": [
    "# Stage 2: TripoSR - Initial 3D Reconstruction\n",
    "\n",
    "**Input:** Single image  \n",
    "**Output:** 3D mesh + Gaussian point cloud  \n",
    "**Time:** ~30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0772717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone TripoSR\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "TRIPOSR_PATH = Path(\"/content/TripoSR\")\n",
    "\n",
    "if not TRIPOSR_PATH.exists():\n",
    "    print(\"üì• Cloning TripoSR...\")\n",
    "    !git clone https://github.com/VAST-AI-Research/TripoSR.git {TRIPOSR_PATH}\n",
    "\n",
    "sys.path.insert(0, str(TRIPOSR_PATH))\n",
    "os.chdir(TRIPOSR_PATH)\n",
    "print(f\"‚úÖ TripoSR ready at {TRIPOSR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tsr.system import TSR\n",
    "from tsr.utils import remove_background, resize_foreground\n",
    "import rembg\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî∑ STAGE 2: TripoSR 3D Reconstruction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüì• Loading TripoSR model...\")\n",
    "triposr_model = TSR.from_pretrained(\n",
    "    \"stabilityai/TripoSR\",\n",
    "    config_name=\"config.yaml\",\n",
    "    weight_name=\"model.ckpt\",\n",
    ")\n",
    "triposr_model.renderer.set_chunk_size(8192)\n",
    "triposr_model.to(device)\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# Preprocess image\n",
    "print(\"\\nüîß Preprocessing image...\")\n",
    "input_img = Image.open(INPUT_PATH)\n",
    "rembg_session = rembg.new_session()\n",
    "processed_img = remove_background(input_img, rembg_session)\n",
    "processed_img = resize_foreground(processed_img, 0.85)\n",
    "\n",
    "# ‚úÖ FIXED: Save RGBA for SyncDreamer, create RGB for TripoSR\n",
    "# SyncDreamer's prepare_inputs needs RGBA with alpha channel\n",
    "processed_img.save(DIRS['triposr'] / \"processed_input.png\")\n",
    "\n",
    "# Create RGB version for TripoSR (it expects 3 channels)\n",
    "img_np = np.array(processed_img).astype(np.float32) / 255.0\n",
    "img_np_rgb = img_np[:, :, :3] * img_np[:, :, 3:4] + (1 - img_np[:, :, 3:4]) * 0.5\n",
    "processed_img_rgb = Image.fromarray((img_np_rgb * 255.0).astype(np.uint8))\n",
    "processed_img_rgb.save(DIRS['triposr'] / \"processed_input_rgb.png\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\nüöÄ Running TripoSR...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: Use RGB version for TripoSR (it expects 3 channels, not 4)\n",
    "with torch.no_grad():\n",
    "    scene_codes = triposr_model([processed_img_rgb], device=device)\n",
    "    meshes = triposr_model.extract_mesh(scene_codes, has_vertex_color=True, resolution=MC_RESOLUTION)\n",
    "\n",
    "mesh = meshes[0]\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Mesh generated in {elapsed:.2f}s\")\n",
    "print(f\"   Vertices: {len(mesh.vertices):,}\")\n",
    "print(f\"   Faces: {len(mesh.faces):,}\")\n",
    "\n",
    "# Save mesh - OBJ always works, GLB may fail with NumPy compatibility issues\n",
    "mesh.export(str(DIRS['triposr'] / \"mesh.obj\"))\n",
    "print(f\"‚úÖ Saved OBJ: {DIRS['triposr'] / 'mesh.obj'}\")\n",
    "\n",
    "try:\n",
    "    mesh.export(str(DIRS['triposr'] / \"mesh.glb\"))\n",
    "    print(f\"‚úÖ Saved GLB: {DIRS['triposr'] / 'mesh.glb'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GLB export failed (NumPy compatibility): {e}\")\n",
    "    print(f\"   Continuing with OBJ format only\")\n",
    "\n",
    "print(f\"\\nüìÅ Mesh saved to {DIRS['triposr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70122c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mesh to Gaussian PLY\n",
    "import gc\n",
    "import numpy as np\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def mesh_to_gaussian_ply(mesh, output_path, num_samples=100000):\n",
    "    \"\"\"Convert mesh to Gaussian Splat format with better initialization.\"\"\"\n",
    "    print(f\"\\nüîÑ Sampling {num_samples:,} points...\")\n",
    "    \n",
    "    points, face_indices = mesh.sample(num_samples, return_index=True)\n",
    "    \n",
    "    if mesh.visual.vertex_colors is not None:\n",
    "        face_vertices = mesh.faces[face_indices]\n",
    "        vertex_colors = mesh.visual.vertex_colors[:, :3] / 255.0\n",
    "        colors = vertex_colors[face_vertices].mean(axis=1)\n",
    "    else:\n",
    "        colors = np.ones((num_samples, 3)) * 0.5\n",
    "    \n",
    "    num_points = len(points)\n",
    "    xyz = points.astype(np.float32)\n",
    "    \n",
    "    # ‚ö†Ô∏è IMPORTANT: Estimate appropriate scale based on point density\n",
    "    # Calculate average nearest neighbor distance to set scale\n",
    "    from scipy.spatial import cKDTree\n",
    "    tree = cKDTree(xyz[:min(10000, len(xyz))])  # Use subset for speed\n",
    "    distances, _ = tree.query(xyz[:min(10000, len(xyz))], k=2)  # k=2 to get nearest neighbor (excluding self)\n",
    "    avg_nn_distance = distances[:, 1].mean()  # Second column is nearest neighbor\n",
    "    \n",
    "    print(f\"   Average point spacing: {avg_nn_distance:.4f}\")\n",
    "    \n",
    "    # Color encoding: Convert RGB to SH DC coefficient\n",
    "    C0 = 0.28209479177387814\n",
    "    features_dc = ((colors - 0.5) / C0).astype(np.float32)\n",
    "    features_rest = np.zeros((num_points, 45), dtype=np.float32)\n",
    "    \n",
    "    # ‚ö†Ô∏è CRITICAL FIX: Better initial parameters\n",
    "    # Opacity: sigmoid(2.0) ‚âà 0.88, good starting point (visible but not saturated)\n",
    "    opacities = np.ones((num_points, 1), dtype=np.float32) * 2.0\n",
    "    \n",
    "    # ‚ö†Ô∏è CRITICAL FIX: Scale based on actual point spacing\n",
    "    # exp(scale_raw) = actual_scale, so scale_raw = log(actual_scale)\n",
    "    # We want Gaussians to slightly overlap, so use 1.5x the average spacing\n",
    "    target_scale = avg_nn_distance * 1.5\n",
    "    scale_raw = np.log(max(target_scale, 0.001))  # Prevent log(0)\n",
    "    print(f\"   Initial Gaussian scale: {target_scale:.4f} (raw={scale_raw:.2f})\")\n",
    "    \n",
    "    scales = np.ones((num_points, 3), dtype=np.float32) * scale_raw\n",
    "    \n",
    "    # ‚úÖ gsplat uses wxyz quaternion convention: rot_0=w, rot_1=x, rot_2=y, rot_3=z\n",
    "    rotations = np.zeros((num_points, 4), dtype=np.float32)\n",
    "    rotations[:, 0] = 1.0  # w=1 (identity rotation in wxyz format)\n",
    "    \n",
    "    dtype_full = [\n",
    "        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "        ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4'),\n",
    "    ]\n",
    "    for i in range(45):\n",
    "        dtype_full.append((f'f_rest_{i}', 'f4'))\n",
    "    dtype_full.extend([\n",
    "        ('opacity', 'f4'),\n",
    "        ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),\n",
    "        ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4'),\n",
    "    ])\n",
    "    \n",
    "    elements = np.zeros(num_points, dtype=dtype_full)\n",
    "    elements['x'] = xyz[:, 0]\n",
    "    elements['y'] = xyz[:, 1]\n",
    "    elements['z'] = xyz[:, 2]\n",
    "    elements['f_dc_0'] = features_dc[:, 0]\n",
    "    elements['f_dc_1'] = features_dc[:, 1]\n",
    "    elements['f_dc_2'] = features_dc[:, 2]\n",
    "    for i in range(45):\n",
    "        elements[f'f_rest_{i}'] = features_rest[:, i]\n",
    "    elements['opacity'] = opacities[:, 0]\n",
    "    elements['scale_0'] = scales[:, 0]\n",
    "    elements['scale_1'] = scales[:, 1]\n",
    "    elements['scale_2'] = scales[:, 2]\n",
    "    elements['rot_0'] = rotations[:, 0]\n",
    "    elements['rot_1'] = rotations[:, 1]\n",
    "    elements['rot_2'] = rotations[:, 2]\n",
    "    elements['rot_3'] = rotations[:, 3]\n",
    "    \n",
    "    el = PlyElement.describe(elements, 'vertex')\n",
    "    PlyData([el]).write(output_path)\n",
    "    print(f\"‚úÖ Saved: {output_path}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä Gaussian Initialization Summary:\")\n",
    "    print(f\"   Points: {num_points:,}\")\n",
    "    print(f\"   Position range: X[{xyz[:,0].min():.3f}, {xyz[:,0].max():.3f}]\")\n",
    "    print(f\"   Scale (exp): {np.exp(scale_raw):.4f}\")\n",
    "    print(f\"   Opacity (sigmoid): {1/(1+np.exp(-2.0)):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fc9c3",
   "metadata": {},
   "source": [
    "# Stage 3: SyncDreamer - Multi-View Generation\n",
    "\n",
    "**Input:** Processed image  \n",
    "**Output:** 16 consistent multi-view images  \n",
    "**Time:** ~2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17879096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé® STAGE 3: SyncDreamer Multi-View Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clone SyncDreamer\n",
    "SYNCDREAMER_PATH = Path(\"/content/SyncDreamer\")\n",
    "\n",
    "if not SYNCDREAMER_PATH.exists():\n",
    "    print(\"üì• Cloning SyncDreamer...\")\n",
    "    !git clone https://github.com/liuyuan-pal/SyncDreamer.git {SYNCDREAMER_PATH}\n",
    "\n",
    "# Download checkpoints\n",
    "CKPT_DIR = SYNCDREAMER_PATH / \"ckpt\"\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "!apt -y install -qq aria2\n",
    "\n",
    "CHECKPOINTS = {\n",
    "    \"syncdreamer-pretrain.ckpt\": \"https://huggingface.co/camenduru/SyncDreamer/resolve/main/syncdreamer-pretrain.ckpt\",\n",
    "    \"ViT-L-14.pt\": \"https://huggingface.co/camenduru/SyncDreamer/resolve/main/ViT-L-14.pt\"\n",
    "}\n",
    "\n",
    "for fname, url in CHECKPOINTS.items():\n",
    "    fpath = CKPT_DIR / fname\n",
    "    if not fpath.exists():\n",
    "        print(f\"üì• Downloading {fname}...\")\n",
    "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d \"{CKPT_DIR}\" -o \"{fname}\"\n",
    "    else:\n",
    "        print(f\"‚úÖ {fname} exists\")\n",
    "\n",
    "sys.path.insert(0, str(SYNCDREAMER_PATH))\n",
    "os.chdir(SYNCDREAMER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: Aggressive memory cleanup before loading SyncDreamer\n",
    "# T4 has limited RAM (~12GB on Colab free tier)\n",
    "print(\"üßπ Clearing memory before loading SyncDreamer...\")\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Check available memory\n",
    "import psutil\n",
    "ram_available = psutil.virtual_memory().available / 1024**3\n",
    "print(f\"   Available RAM: {ram_available:.1f} GB\")\n",
    "if ram_available < 5:\n",
    "    print(\"   ‚ö†Ô∏è Low RAM warning! Consider restarting runtime.\")\n",
    "\n",
    "# Load SyncDreamer model\n",
    "print(\"\\nüì• Loading SyncDreamer model...\")\n",
    "\n",
    "config_path = SYNCDREAMER_PATH / \"configs\" / \"syncdreamer.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "# Instantiate model from config\n",
    "syncdreamer_model = instantiate_from_config(config.model)\n",
    "\n",
    "# Load pretrained weights with MEMORY-MAPPED approach\n",
    "ckpt_path = CKPT_DIR / \"syncdreamer-pretrain.ckpt\"\n",
    "print(f\"   Loading checkpoint: {ckpt_path}\")\n",
    "\n",
    "# ‚ö†Ô∏è MEMORY FIX: Use mmap=True to avoid loading entire file into RAM\n",
    "# This memory-maps the file instead of loading it all at once\n",
    "try:\n",
    "    # Try mmap first (PyTorch 2.1+)\n",
    "    print(\"   Using memory-mapped loading (mmap=True)...\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", mmap=True, weights_only=False)\n",
    "except TypeError:\n",
    "    # Fallback for older PyTorch without mmap support\n",
    "    print(\"   Fallback: Standard loading...\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# Extract state dict\n",
    "if \"state_dict\" in checkpoint:\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# Load weights\n",
    "print(\"   Loading state dict into model...\")\n",
    "missing, unexpected = syncdreamer_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: Delete checkpoint from RAM immediately\n",
    "print(\"   Cleaning up checkpoint from RAM...\")\n",
    "del checkpoint\n",
    "del state_dict\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "if missing:\n",
    "    print(f\"   ‚ö†Ô∏è Missing keys: {len(missing)}\")\n",
    "if unexpected:\n",
    "    print(f\"   ‚ö†Ô∏è Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "# Check RAM after loading\n",
    "ram_after = psutil.virtual_memory().available / 1024**3\n",
    "print(f\"   RAM after loading: {ram_after:.1f} GB\")\n",
    "\n",
    "# Move to GPU\n",
    "print(\"   Moving model to GPU...\")\n",
    "syncdreamer_model = syncdreamer_model.cuda().eval()\n",
    "\n",
    "# Final cleanup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verify model is ready\n",
    "print(f\"\\n‚úÖ SyncDreamer loaded!\")\n",
    "print(f\"   Model type: {type(syncdreamer_model).__name__}\")\n",
    "print(f\"   GPU memory: {torch.cuda.memory_allocated()/1024**3:.1f} GB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.models.diffusion.sync_dreamer import SyncDDIMSampler\n",
    "from ldm.util import prepare_inputs  # CRITICAL: Use official data preparation\n",
    "\n",
    "# ‚úÖ FIXED: Camera configuration MUST match SyncDreamer training data\n",
    "# SyncDreamer generates 16 views at FIXED 30¬∞ elevation, azimuths spaced 22.5¬∞ apart\n",
    "ELEVATIONS = [30.0] * 16  # All 16 views at 30¬∞ elevation\n",
    "AZIMUTHS = [i * 22.5 for i in range(16)]  # 0¬∞, 22.5¬∞, 45¬∞, ..., 337.5¬∞\n",
    "RADIUS = 1.5\n",
    "\n",
    "# ‚úÖ FIXED: Use official prepare_inputs function for proper data preparation\n",
    "# This handles alpha channel, CLIP embedding, and proper normalization\n",
    "processed_path = DIRS['triposr'] / \"processed_input.png\"\n",
    "\n",
    "INPUT_ELEVATION = 30.0  # Assume front view at 30 degrees\n",
    "CROP_SIZE = 200         # Crop foreground to this size\n",
    "\n",
    "print(f\"üì∏ Preparing input: {processed_path}\")\n",
    "print(f\"   Input elevation: {INPUT_ELEVATION}¬∞\")\n",
    "print(f\"   Crop size: {CROP_SIZE}\")\n",
    "\n",
    "# Verify image has alpha channel (required by prepare_inputs)\n",
    "img_check = Image.open(str(processed_path))\n",
    "print(f\"   Image format: {img_check.mode} (channels: {len(img_check.getbands())})\")\n",
    "if img_check.mode != 'RGBA':\n",
    "    print(\"   ‚ö†Ô∏è Converting to RGBA...\")\n",
    "    img_check = img_check.convert('RGBA')\n",
    "    img_check.save(str(processed_path))\n",
    "img_check.close()\n",
    "\n",
    "# Use official SyncDreamer data preparation\n",
    "data = prepare_inputs(str(processed_path), INPUT_ELEVATION, CROP_SIZE)\n",
    "\n",
    "# Move to GPU and add batch dimension\n",
    "for k, v in data.items():\n",
    "    data[k] = v.unsqueeze(0).cuda()\n",
    "    print(f\"   {k}: {data[k].shape}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Input prepared using official prepare_inputs()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2618cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SyncDreamer inference\n",
    "print(\"\\nüöÄ Running SyncDreamer (this takes ~2-3 minutes)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Settings\n",
    "SAMPLE_STEPS = 50\n",
    "CFG_SCALE = 2.0\n",
    "\n",
    "sampler = SyncDDIMSampler(syncdreamer_model, SAMPLE_STEPS)\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # ‚úÖ FIXED: Data already prepared correctly by prepare_inputs()\n",
    "        # Run synchronized multi-view generation\n",
    "        x_sample = syncdreamer_model.sample(\n",
    "            sampler, \n",
    "            data, \n",
    "            CFG_SCALE, \n",
    "            BATCH_VIEW_NUM\n",
    "        )\n",
    "        # x_sample shape: (B, N, C, H, W) where N=16 views\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"‚ö†Ô∏è OOM Error! Reducing batch size and retrying...\")\n",
    "        clear_gpu()\n",
    "        BATCH_VIEW_NUM = max(1, BATCH_VIEW_NUM // 2)\n",
    "        print(f\"   New BATCH_VIEW_NUM: {BATCH_VIEW_NUM}\")\n",
    "        \n",
    "        sampler = SyncDDIMSampler(syncdreamer_model, SAMPLE_STEPS)\n",
    "        with torch.no_grad():\n",
    "            x_sample = syncdreamer_model.sample(sampler, data, CFG_SCALE, BATCH_VIEW_NUM)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ SyncDreamer completed in {elapsed/60:.1f} minutes\")\n",
    "print(f\"   Output shape: {x_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d504fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save multi-view images\n",
    "print(\"\\nüíæ Saving multi-view images...\")\n",
    "\n",
    "# Convert samples to images: [-1,1] -> [0,1]\n",
    "samples = (x_sample.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "syncdreamer_views = []\n",
    "\n",
    "B, N, C, H, W = samples.shape\n",
    "print(f\"   Processing {N} views at {H}x{W}\")\n",
    "\n",
    "for i in range(N):\n",
    "    img_tensor = samples[0, i]  # (C, H, W)\n",
    "    img_np = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    img_pil = Image.fromarray(img_np)\n",
    "    \n",
    "    # Save individual view\n",
    "    elev = int(ELEVATIONS[i])\n",
    "    azim = int(AZIMUTHS[i])\n",
    "    save_path = DIRS['syncdreamer'] / f\"view_{i:02d}_e{elev}_a{azim}.png\"\n",
    "    img_pil.save(save_path)\n",
    "    syncdreamer_views.append(img_pil)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(syncdreamer_views)} views to {DIRS['syncdreamer']}\")\n",
    "\n",
    "# Display grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(syncdreamer_views[i])\n",
    "    ax.set_title(f\"E={int(ELEVATIONS[i])}¬∞ A={int(AZIMUTHS[i])}¬∞\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"SyncDreamer: 16 Multi-View Images\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS['syncdreamer'] / \"grid.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Cleanup SyncDreamer to free VRAM\n",
    "del syncdreamer_model, sampler, x_sample, samples\n",
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456650c8",
   "metadata": {},
   "source": [
    "# Stage 4: SDXL Enhancement (Optional)\n",
    "\n",
    "**Input:** Multi-view images  \n",
    "**Output:** Enhanced multi-view images  \n",
    "**Time:** ~1 minute per image\n",
    "\n",
    "Skip this stage if you want faster results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_ENHANCEMENT = False  # Set to True to skip this stage\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: Check RAM BEFORE attempting to load SDXL\n",
    "# SDXL UNet alone requires ~5-6GB RAM to load, plus existing Python overhead\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Aggressive cleanup first\n",
    "for _ in range(10):\n",
    "    gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "ram_available = psutil.virtual_memory().available / 1024**3\n",
    "ram_total = psutil.virtual_memory().total / 1024**3\n",
    "\n",
    "print(f\"\\nüìä Memory Status:\")\n",
    "print(f\"   Available RAM: {ram_available:.1f} GB / {ram_total:.1f} GB\")\n",
    "print(f\"   SDXL UNet requires: ~6 GB RAM to load\")\n",
    "\n",
    "# Auto-skip if RAM is insufficient (need ~8GB free to be safe)\n",
    "if ram_available < 7:\n",
    "    print(f\"\\n‚ö†Ô∏è AUTO-SKIP: Insufficient RAM for SDXL ({ram_available:.1f} GB < 7 GB)\")\n",
    "    print(\"   T4 GPUs on Colab free tier often have limited RAM.\")\n",
    "    print(\"   Skipping SDXL enhancement to prevent crash.\")\n",
    "    SKIP_ENHANCEMENT = True\n",
    "\n",
    "if not SKIP_ENHANCEMENT:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ú® STAGE 4: SDXL Lightning Enhancement\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save view list to disk temporarily, then delete from RAM\n",
    "    import pickle\n",
    "    views_cache_path = DIRS['syncdreamer'] / \"_views_cache.pkl\"\n",
    "    with open(views_cache_path, 'wb') as f:\n",
    "        pickle.dump(syncdreamer_views, f)\n",
    "    del syncdreamer_views\n",
    "    \n",
    "    # Delete any lingering objects\n",
    "    for var_name in ['data', 'x_sample', 'samples', 'sampler']:\n",
    "        try:\n",
    "            exec(f'del {var_name}')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Aggressive garbage collection\n",
    "    for _ in range(10):\n",
    "        gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    from diffusers import StableDiffusionXLImg2ImgPipeline, AutoencoderKL, EulerDiscreteScheduler\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    from safetensors.torch import load_file\n",
    "    \n",
    "    print(\"\\nüì• Loading SDXL Lightning...\")\n",
    "    print(\"   Using single-file LoRA method (lower RAM usage)\")\n",
    "    \n",
    "    base_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    repo = \"ByteDance/SDXL-Lightning\"\n",
    "    \n",
    "    try:\n",
    "        # ‚úÖ MEMORY-OPTIMIZED: Load pipeline with low_cpu_mem_usage\n",
    "        print(\"   Loading base pipeline (this may take a minute)...\")\n",
    "        pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "            base_model,\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "            use_safetensors=True,\n",
    "            low_cpu_mem_usage=True,  # ‚ö†Ô∏è CRITICAL: Reduces RAM during loading\n",
    "        )\n",
    "        \n",
    "        # Apply fp16-fixed VAE\n",
    "        print(\"   Loading fp16-fixed VAE...\")\n",
    "        pipe.vae = AutoencoderKL.from_pretrained(\n",
    "            \"madebyollin/sdxl-vae-fp16-fix\",\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        \n",
    "        # Download and load Lightning LoRA (much smaller than full UNet swap)\n",
    "        print(\"   Applying Lightning LoRA weights...\")\n",
    "        lora_path = hf_hub_download(repo, \"sdxl_lightning_4step_lora.safetensors\")\n",
    "        pipe.load_lora_weights(lora_path)\n",
    "        pipe.fuse_lora()  # Fuse for faster inference\n",
    "        \n",
    "        # Use correct scheduler for Lightning\n",
    "        pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
    "            pipe.scheduler.config, \n",
    "            timestep_spacing=\"trailing\"\n",
    "        )\n",
    "        \n",
    "        # Move to GPU\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "        \n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"‚úÖ SDXL Lightning loaded (LoRA method)!\")\n",
    "        print(\"   ‚ö†Ô∏è Remember: guidance_scale MUST be 0 for Lightning\")\n",
    "        \n",
    "        # Reload syncdreamer_views from cache\n",
    "        print(\"   Loading cached views...\")\n",
    "        with open(views_cache_path, 'rb') as f:\n",
    "            syncdreamer_views = pickle.load(f)\n",
    "        views_cache_path.unlink()  # Delete cache file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load SDXL Lightning: {e}\")\n",
    "        print(\"   Falling back to skip enhancement\")\n",
    "        SKIP_ENHANCEMENT = True\n",
    "        \n",
    "        # Reload syncdreamer_views if enhancement failed\n",
    "        try:\n",
    "            with open(views_cache_path, 'rb') as f:\n",
    "                syncdreamer_views = pickle.load(f)\n",
    "            views_cache_path.unlink()\n",
    "        except:\n",
    "            # Reload from disk as fallback\n",
    "            syncdreamer_views = []\n",
    "            for i in range(16):\n",
    "                img_path = DIRS['syncdreamer'] / f\"view_{i:02d}_e30_a{int(i*22.5)}.png\"\n",
    "                syncdreamer_views.append(Image.open(img_path))\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è Skipping SDXL enhancement stage\")\n",
    "    print(\"   Using original SyncDreamer views (still high quality!)\")\n",
    "    \n",
    "    # Make sure syncdreamer_views is available for next stage\n",
    "    if 'syncdreamer_views' not in dir() or syncdreamer_views is None:\n",
    "        print(\"   Reloading views from disk...\")\n",
    "        syncdreamer_views = []\n",
    "        for i in range(16):\n",
    "            img_path = DIRS['syncdreamer'] / f\"view_{i:02d}_e30_a{int(i*22.5)}.png\"\n",
    "            syncdreamer_views.append(Image.open(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_ENHANCEMENT:\n",
    "    # Enhance select views (not all 16 to save time)\n",
    "    VIEWS_TO_ENHANCE = [0, 4, 8, 12]  # Only 4 views to reduce memory pressure\n",
    "    \n",
    "    print(f\"\\nüöÄ Enhancing {len(VIEWS_TO_ENHANCE)} views...\")\n",
    "    \n",
    "    enhanced_views = syncdreamer_views.copy()  # Start with original\n",
    "    \n",
    "    prompt = \"highly detailed 3D render, professional studio lighting, sharp textures, photorealistic, 8k quality\"\n",
    "    negative_prompt = \"blurry, low quality, artifacts, noise, watermark, text\"\n",
    "    \n",
    "    for i, view_idx in enumerate(VIEWS_TO_ENHANCE):\n",
    "        print(f\"  Enhancing view {view_idx} ({i+1}/{len(VIEWS_TO_ENHANCE)})...\")\n",
    "        \n",
    "        # Resize input for SDXL (works best at 512-1024)\n",
    "        input_img = syncdreamer_views[view_idx].resize((512, 512), Image.LANCZOS)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=input_img,\n",
    "                strength=0.35,  # Lower = preserve more original structure\n",
    "                num_inference_steps=4,  # Lightning uses 4 steps\n",
    "                guidance_scale=0,  # Lightning uses CFG=0\n",
    "            ).images[0]\n",
    "        \n",
    "        # Resize back to match SyncDreamer output size\n",
    "        result_resized = result.resize((256, 256), Image.LANCZOS)\n",
    "        enhanced_views[view_idx] = result_resized\n",
    "        result.save(DIRS['enhanced'] / f\"enhanced_{view_idx:02d}.png\")\n",
    "        \n",
    "        # Clear VRAM between images\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Enhanced views saved to {DIRS['enhanced']}\")\n",
    "    \n",
    "    # Cleanup SDXL to free VRAM for gsplat\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"üßπ SDXL cleaned up\")\n",
    "else:\n",
    "    # Use original SyncDreamer views (already high quality)\n",
    "    enhanced_views = syncdreamer_views\n",
    "    print(\"‚úÖ Using original SyncDreamer views (no enhancement)\")\n",
    "    print(\"   Note: SyncDreamer views are already high quality for gsplat optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cacdbd",
   "metadata": {},
   "source": [
    "# Stage 5: gsplat Optimization\n",
    "\n",
    "**Input:** Initial Gaussian PLY + Multi-view images  \n",
    "**Output:** Optimized Gaussian Splats  \n",
    "**Time:** ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÆ STAGE 5: gsplat Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import torch.nn as nn\n",
    "from gsplat import rasterization\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# ‚ö†Ô∏è PRE-COMPILE GSPLAT CUDA KERNELS\n",
    "# gsplat uses JIT compilation - first call takes 5-10 minutes on T4\n",
    "# We do a dummy render here so the compilation happens with a clear progress message\n",
    "print(\"\\n‚è≥ Pre-compiling gsplat CUDA kernels...\")\n",
    "print(\"   This takes 5-10 minutes on first run (one-time per session)\")\n",
    "print(\"   You'll see 'Setting up CUDA...' - this is normal!\")\n",
    "\n",
    "# Dummy tensors for compilation trigger\n",
    "_dummy_means = torch.zeros(100, 3, device=device)\n",
    "_dummy_quats = torch.tensor([[1, 0, 0, 0]] * 100, dtype=torch.float32, device=device)\n",
    "_dummy_scales = torch.ones(100, 3, device=device) * 0.01\n",
    "_dummy_opacities = torch.ones(100, device=device)\n",
    "_dummy_colors = torch.ones(100, 3, device=device)\n",
    "_dummy_viewmat = torch.eye(4, device=device).unsqueeze(0)\n",
    "_dummy_K = torch.tensor([[128, 0, 64], [0, 128, 64], [0, 0, 1]], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "try:\n",
    "    _ = rasterization(\n",
    "        means=_dummy_means,\n",
    "        quats=_dummy_quats,\n",
    "        scales=_dummy_scales,\n",
    "        opacities=_dummy_opacities,\n",
    "        colors=_dummy_colors,\n",
    "        viewmats=_dummy_viewmat,\n",
    "        Ks=_dummy_K,\n",
    "        width=128,\n",
    "        height=128,\n",
    "        packed=False,\n",
    "        render_mode=\"RGB\",\n",
    "    )\n",
    "    print(\"‚úÖ gsplat CUDA kernels compiled!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Pre-compilation note: {e}\")\n",
    "\n",
    "# Cleanup dummy tensors\n",
    "del _dummy_means, _dummy_quats, _dummy_scales, _dummy_opacities, _dummy_colors, _dummy_viewmat, _dummy_K\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Image size for rendering (matches SyncDreamer output)\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "# Load initial Gaussians from PLY\n",
    "def load_gaussian_ply(path):\n",
    "    \"\"\"Load Gaussian parameters from PLY file.\"\"\"\n",
    "    plydata = PlyData.read(path)\n",
    "    vertex = plydata['vertex']\n",
    "    \n",
    "    xyz = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=-1)\n",
    "    f_dc = np.stack([vertex['f_dc_0'], vertex['f_dc_1'], vertex['f_dc_2']], axis=-1)\n",
    "    \n",
    "    # Load f_rest if present\n",
    "    f_rest_names = [f'f_rest_{i}' for i in range(45)]\n",
    "    available_f_rest = [name for name in f_rest_names if name in vertex.data.dtype.names]\n",
    "    if available_f_rest:\n",
    "        f_rest = np.stack([vertex[name] for name in available_f_rest], axis=-1)\n",
    "    else:\n",
    "        f_rest = np.zeros((len(xyz), 45), dtype=np.float32)\n",
    "    \n",
    "    # ‚úÖ FIXED: Ensure opacity is 1D (N,) - gsplat requires this shape\n",
    "    opacity = np.asarray(vertex['opacity'], dtype=np.float32).flatten()\n",
    "    scales = np.stack([vertex['scale_0'], vertex['scale_1'], vertex['scale_2']], axis=-1)\n",
    "    rotations = np.stack([vertex['rot_0'], vertex['rot_1'], vertex['rot_2'], vertex['rot_3']], axis=-1)\n",
    "    \n",
    "    return {\n",
    "        'xyz': torch.tensor(xyz, dtype=torch.float32),\n",
    "        'f_dc': torch.tensor(f_dc, dtype=torch.float32),\n",
    "        'f_rest': torch.tensor(f_rest, dtype=torch.float32),\n",
    "        'opacity': torch.tensor(opacity, dtype=torch.float32),  # Shape: (N,)\n",
    "        'scales': torch.tensor(scales, dtype=torch.float32),\n",
    "        'rotations': torch.tensor(rotations, dtype=torch.float32),\n",
    "    }\n",
    "\n",
    "gaussians = load_gaussian_ply(str(INITIAL_PLY_PATH))\n",
    "print(f\"\\n‚úÖ Loaded {len(gaussians['xyz']):,} Gaussians\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianModel(nn.Module):\n",
    "    def __init__(self, gaussians):\n",
    "        super().__init__()\n",
    "        self.xyz = nn.Parameter(gaussians['xyz'].clone())\n",
    "        self.f_dc = nn.Parameter(gaussians['f_dc'].clone())\n",
    "        self.f_rest = nn.Parameter(gaussians['f_rest'].clone())\n",
    "        # ‚úÖ FIXED: Ensure opacity is 1D (N,) - gsplat requires this shape\n",
    "        opacity_tensor = gaussians['opacity'].clone()\n",
    "        if opacity_tensor.dim() > 1:\n",
    "            opacity_tensor = opacity_tensor.squeeze(-1)\n",
    "        self.opacity_raw = nn.Parameter(opacity_tensor)\n",
    "        self.scales_raw = nn.Parameter(gaussians['scales'].clone())\n",
    "        self.rotations = nn.Parameter(gaussians['rotations'].clone())\n",
    "        \n",
    "    @property\n",
    "    def opacity(self):\n",
    "        # ‚úÖ FIXED: Returns (N,) shape - required by gsplat.rasterization()\n",
    "        return torch.sigmoid(self.opacity_raw)\n",
    "    \n",
    "    @property\n",
    "    def scales(self):\n",
    "        return torch.exp(self.scales_raw)\n",
    "    \n",
    "    def get_colors(self):\n",
    "        C0 = 0.28209479177387814\n",
    "        return 0.5 + C0 * self.f_dc\n",
    "    \n",
    "    def forward(self):\n",
    "        return {\n",
    "            'xyz': self.xyz,\n",
    "            'colors': self.get_colors(),\n",
    "            'opacity': self.opacity,\n",
    "            'scales': self.scales,\n",
    "            'rotations': self.rotations / (self.rotations.norm(dim=-1, keepdim=True) + 1e-8),\n",
    "        }\n",
    "\n",
    "model = GaussianModel(gaussians).to(device)\n",
    "print(f\"‚úÖ Model: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera system matching SyncDreamer conventions\n",
    "# SyncDreamer uses: Y-up, camera looks at origin, radius ~1.5\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: First check where the Gaussians actually are\n",
    "with torch.no_grad():\n",
    "    xyz = model.xyz.cpu().numpy()\n",
    "    gaussian_center = xyz.mean(axis=0)\n",
    "    gaussian_extent = (xyz.max(axis=0) - xyz.min(axis=0)).max()\n",
    "    \n",
    "print(f\"üìä Gaussian cloud analysis:\")\n",
    "print(f\"   Center: ({gaussian_center[0]:.3f}, {gaussian_center[1]:.3f}, {gaussian_center[2]:.3f})\")\n",
    "print(f\"   Extent: {gaussian_extent:.3f}\")\n",
    "\n",
    "# ‚ö†Ô∏è FIX: If Gaussians are not at origin, either:\n",
    "# 1. Recenter the Gaussians (recommended)\n",
    "# 2. Or adjust camera look_at point\n",
    "\n",
    "# Option 1: Recenter Gaussians to origin (better approach)\n",
    "if np.linalg.norm(gaussian_center) > 0.1:  # If center is more than 0.1 from origin\n",
    "    print(f\"\\nüîß Recentering Gaussians to origin...\")\n",
    "    with torch.no_grad():\n",
    "        model.xyz.data -= torch.tensor(gaussian_center, device=device, dtype=torch.float32)\n",
    "    gaussian_center = np.array([0.0, 0.0, 0.0])\n",
    "    print(f\"   New center: (0, 0, 0)\")\n",
    "\n",
    "# Adjust radius based on actual model size\n",
    "# Camera should be ~2.5x the model extent away for good framing\n",
    "RADIUS = max(gaussian_extent * 2.5, 1.5)\n",
    "print(f\"   Camera radius: {RADIUS:.3f}\")\n",
    "\n",
    "def create_camera_pose(elevation_deg, azimuth_deg, radius=1.5, look_at=None):\n",
    "    \"\"\"Create world-to-camera matrix for given elevation and azimuth.\"\"\"\n",
    "    if look_at is None:\n",
    "        look_at = np.array([0, 0, 0])\n",
    "    \n",
    "    elev = math.radians(elevation_deg)\n",
    "    azim = math.radians(azimuth_deg)\n",
    "    \n",
    "    # Camera position in spherical coordinates (Y-up convention)\n",
    "    x = radius * math.cos(elev) * math.sin(azim)\n",
    "    y = radius * math.sin(elev)\n",
    "    z = radius * math.cos(elev) * math.cos(azim)\n",
    "    \n",
    "    cam_pos = np.array([x, y, z]) + look_at  # Offset by look_at point\n",
    "    up = np.array([0, 1, 0])  # Y-up\n",
    "    \n",
    "    # Construct camera basis\n",
    "    forward = look_at - cam_pos\n",
    "    forward = forward / (np.linalg.norm(forward) + 1e-8)\n",
    "    right = np.cross(forward, up)\n",
    "    right = right / (np.linalg.norm(right) + 1e-8)\n",
    "    up_new = np.cross(right, forward)\n",
    "    \n",
    "    # World-to-camera transformation\n",
    "    # R rotates world to camera, t translates\n",
    "    w2c = np.eye(4, dtype=np.float32)\n",
    "    w2c[0, :3] = right\n",
    "    w2c[1, :3] = up_new\n",
    "    w2c[2, :3] = -forward  # Camera looks along -Z\n",
    "    w2c[:3, 3] = -w2c[:3, :3] @ cam_pos\n",
    "    \n",
    "    return w2c\n",
    "\n",
    "def get_intrinsics(fov_deg=49.1, image_size=256):\n",
    "    \"\"\"Get camera intrinsics matrix. FOV ~49.1 matches SyncDreamer.\"\"\"\n",
    "    fov_rad = math.radians(fov_deg)\n",
    "    focal = image_size / (2 * math.tan(fov_rad / 2))\n",
    "    \n",
    "    K = np.array([\n",
    "        [focal, 0, image_size / 2],\n",
    "        [0, focal, image_size / 2],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "    return K\n",
    "\n",
    "# Pre-compute all camera poses (using SyncDreamer camera parameters)\n",
    "# Now looking at actual Gaussian center\n",
    "camera_poses = [create_camera_pose(e, a, radius=RADIUS, look_at=gaussian_center) \n",
    "                for e, a in zip(ELEVATIONS, AZIMUTHS)]\n",
    "intrinsics = get_intrinsics(fov_deg=49.1, image_size=IMAGE_SIZE)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(camera_poses)} camera poses\")\n",
    "print(f\"   Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"   Intrinsics: focal={intrinsics[0,0]:.1f}, center=({intrinsics[0,2]:.0f}, {intrinsics[1,2]:.0f})\")\n",
    "\n",
    "# ‚ö†Ô∏è DEBUG: Verify camera can see the Gaussians\n",
    "with torch.no_grad():\n",
    "    test_render, test_alpha = render_gaussians(model, camera_poses[0], intrinsics, IMAGE_SIZE)\n",
    "    test_mean = test_render.mean().item()\n",
    "    print(f\"\\nüîç Camera verification render:\")\n",
    "    print(f\"   Mean brightness: {test_mean:.4f}\")\n",
    "    if test_mean < 0.01:\n",
    "        print(\"   ‚ö†Ô∏è Very dark - may need scale/opacity adjustment\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Gaussians are visible to camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a425de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_gaussians(model, w2c, K, image_size):\n",
    "    \"\"\"Render Gaussian splats from a camera viewpoint.\"\"\"\n",
    "    params = model()\n",
    "    \n",
    "    viewmat = torch.tensor(w2c, dtype=torch.float32, device=device)\n",
    "    K_tensor = torch.tensor(K, dtype=torch.float32, device=device)\n",
    "    \n",
    "    try:\n",
    "        render_colors, render_alphas, info = rasterization(\n",
    "            means=params['xyz'],\n",
    "            quats=params['rotations'],\n",
    "            scales=params['scales'],\n",
    "            opacities=params['opacity'],\n",
    "            colors=params['colors'],\n",
    "            viewmats=viewmat.unsqueeze(0),\n",
    "            Ks=K_tensor.unsqueeze(0),\n",
    "            width=image_size,\n",
    "            height=image_size,\n",
    "            packed=False,\n",
    "            render_mode=\"RGB\",\n",
    "        )\n",
    "        return render_colors[0], render_alphas[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Render error: {e}\")\n",
    "        # Return empty image on error\n",
    "        return torch.zeros(image_size, image_size, 3, device=device), torch.zeros(image_size, image_size, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38273f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================================================\n",
    "# ‚ö†Ô∏è CRITICAL: Verify setup before optimization\n",
    "# ============================================================\n",
    "\n",
    "# Check target images\n",
    "print(\"üîç Pre-optimization Diagnostics:\")\n",
    "print(\"\\nüìä Target Images:\")\n",
    "sample_view = enhanced_views[0]\n",
    "print(f\"   Type: {type(sample_view)}\")\n",
    "print(f\"   Mode: {sample_view.mode}\")\n",
    "print(f\"   Size: {sample_view.size}\")\n",
    "img_array = np.array(sample_view)\n",
    "print(f\"   Array shape: {img_array.shape}\")\n",
    "print(f\"   Value range: [{img_array.min()}, {img_array.max()}]\")\n",
    "print(f\"   Mean value: {img_array.mean():.1f}\")\n",
    "\n",
    "# Check Gaussian initialization\n",
    "with torch.no_grad():\n",
    "    xyz = model.xyz.cpu().numpy()\n",
    "    colors = model.get_colors().cpu().numpy()\n",
    "    opacity = model.opacity.cpu().numpy()\n",
    "    scales = model.scales.cpu().numpy()\n",
    "    \n",
    "print(f\"\\nüìä Initial Gaussians:\")\n",
    "print(f\"   Points: {len(xyz):,}\")\n",
    "print(f\"   XYZ center: ({xyz.mean(0)[0]:.3f}, {xyz.mean(0)[1]:.3f}, {xyz.mean(0)[2]:.3f})\")\n",
    "print(f\"   XYZ extent: {xyz.max() - xyz.min():.3f}\")\n",
    "print(f\"   Opacity: [{opacity.min():.4f}, {opacity.max():.4f}] (mean={opacity.mean():.4f})\")\n",
    "print(f\"   Scales: [{scales.min():.6f}, {scales.max():.6f}] (mean={scales.mean():.6f})\")\n",
    "print(f\"   Colors: [{colors.min():.4f}, {colors.max():.4f}] (mean={colors.mean():.4f})\")\n",
    "\n",
    "print(f\"\\nüìä Camera Setup:\")\n",
    "print(f\"   Radius: {RADIUS}\")\n",
    "print(f\"   Image size: {IMAGE_SIZE}\")\n",
    "print(f\"   Num views: {len(camera_poses)}\")\n",
    "\n",
    "# ============================================================\n",
    "# Prepare target images as tensors\n",
    "# ============================================================\n",
    "target_tensors = []\n",
    "for img in enhanced_views:\n",
    "    img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE), Image.LANCZOS)\n",
    "    # ‚ö†Ô∏è FIX: Ensure RGB (3 channels), not RGBA\n",
    "    if img_resized.mode == 'RGBA':\n",
    "        img_resized = img_resized.convert('RGB')\n",
    "    img_tensor = torch.tensor(np.array(img_resized) / 255.0, dtype=torch.float32, device=device)\n",
    "    # Ensure shape is [H, W, 3]\n",
    "    if img_tensor.dim() == 3 and img_tensor.shape[-1] == 3:\n",
    "        target_tensors.append(img_tensor)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Warning: Unexpected tensor shape {img_tensor.shape}\")\n",
    "        # Force to 3 channels\n",
    "        target_tensors.append(img_tensor[..., :3])\n",
    "\n",
    "print(f\"\\n‚úÖ Prepared {len(target_tensors)} target images at {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"   Target tensor shape: {target_tensors[0].shape}\")\n",
    "print(f\"   Target value range: [{target_tensors[0].min():.3f}, {target_tensors[0].max():.3f}]\")\n",
    "\n",
    "# ============================================================\n",
    "# ‚ö†Ô∏è TEST RENDER: Check if Gaussians are visible BEFORE training\n",
    "# ============================================================\n",
    "print(\"\\nüîç Test render BEFORE optimization...\")\n",
    "with torch.no_grad():\n",
    "    test_render, test_alpha = render_gaussians(model, camera_poses[0], intrinsics, IMAGE_SIZE)\n",
    "    test_np = test_render.detach().cpu().numpy()\n",
    "    print(f\"   Rendered shape: {test_np.shape}\")\n",
    "    print(f\"   Rendered range: [{test_np.min():.4f}, {test_np.max():.4f}]\")\n",
    "    print(f\"   Rendered mean: {test_np.mean():.4f}\")\n",
    "    \n",
    "    if test_np.max() < 0.01:\n",
    "        print(\"\\n   ‚ö†Ô∏è WARNING: Initial render is nearly BLACK!\")\n",
    "        print(\"   This means cameras are not seeing the Gaussians.\")\n",
    "        print(\"   Possible causes:\")\n",
    "        print(\"     1. Gaussians are too small (scales)\")\n",
    "        print(\"     2. Gaussians are invisible (opacity)\")\n",
    "        print(\"     3. Camera is not pointing at Gaussians\")\n",
    "        \n",
    "        # Try to fix by adjusting scale\n",
    "        print(\"\\n   üîß Attempting to fix: Increasing initial scales...\")\n",
    "        model.scales_raw.data += 1.0  # Increase scale by e^1 ‚âà 2.7x\n",
    "        \n",
    "        # Re-test\n",
    "        test_render2, _ = render_gaussians(model, camera_poses[0], intrinsics, IMAGE_SIZE)\n",
    "        test_np2 = test_render2.detach().cpu().numpy()\n",
    "        print(f\"   After scale fix - render mean: {test_np2.mean():.4f}\")\n",
    "        \n",
    "        if test_np2.max() < 0.01:\n",
    "            print(\"   ‚ö†Ô∏è Still black - increasing opacity...\")\n",
    "            model.opacity_raw.data += 2.0  # Higher starting opacity\n",
    "            \n",
    "            test_render3, _ = render_gaussians(model, camera_poses[0], intrinsics, IMAGE_SIZE)\n",
    "            test_np3 = test_render3.detach().cpu().numpy()\n",
    "            print(f\"   After opacity fix - render mean: {test_np3.mean():.4f}\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Initial render looks good!\")\n",
    "\n",
    "# Show test render vs target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(test_render.detach().cpu().numpy())\n",
    "axes[0].set_title(f'Initial Render (mean={test_render.mean():.3f})')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(target_tensors[0].cpu().numpy())\n",
    "axes[1].set_title(f'Target (mean={target_tensors[0].mean():.3f})')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS['gsplat'] / \"render_vs_target.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Optimizer with CONSERVATIVE learning rates\n",
    "# ============================================================\n",
    "# ‚ö†Ô∏è Key insight: Start with smaller LRs to prevent divergence\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.xyz, 'lr': 1e-5, 'name': 'xyz'},           # Very small - positions are already good\n",
    "    {'params': model.f_dc, 'lr': 1e-3, 'name': 'f_dc'},         # Color is important\n",
    "    {'params': model.f_rest, 'lr': 1e-3 / 20, 'name': 'f_rest'},\n",
    "    {'params': model.opacity_raw, 'lr': 1e-2, 'name': 'opacity'},  # Moderate\n",
    "    {'params': model.scales_raw, 'lr': 1e-3, 'name': 'scales'},    # Small - scales are sensitive\n",
    "    {'params': model.rotations, 'lr': 1e-4, 'name': 'rotations'},\n",
    "])\n",
    "\n",
    "# Gentler decay\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.998)\n",
    "\n",
    "# ============================================================\n",
    "# Training loop with better monitoring\n",
    "# ============================================================\n",
    "NUM_ITERATIONS = 1000\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "best_state = None\n",
    "\n",
    "print(f\"\\nüöÄ Starting optimization for {NUM_ITERATIONS} iterations...\")\n",
    "pbar = tqdm(range(NUM_ITERATIONS))\n",
    "\n",
    "for iteration in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample random view\n",
    "    view_idx = np.random.randint(0, 16)\n",
    "    w2c = camera_poses[view_idx]\n",
    "    target = target_tensors[view_idx]\n",
    "    \n",
    "    # Render\n",
    "    rendered, alpha = render_gaussians(model, w2c, intrinsics, IMAGE_SIZE)\n",
    "    \n",
    "    # ‚ö†Ô∏è Check for NaN/Inf early\n",
    "    if torch.isnan(rendered).any() or torch.isinf(rendered).any():\n",
    "        print(f\"\\n‚ö†Ô∏è NaN/Inf detected at iteration {iteration}! Stopping...\")\n",
    "        break\n",
    "    \n",
    "    # L1 loss (more stable than MSE)\n",
    "    l1_loss = F.l1_loss(rendered, target)\n",
    "    loss = l1_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # ‚ö†Ô∏è Check gradients\n",
    "    grad_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_norm += p.grad.norm().item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    \n",
    "    # Skip update if gradients are crazy\n",
    "    if grad_norm > 100:\n",
    "        print(f\"\\n‚ö†Ô∏è Large gradient ({grad_norm:.1f}) at iter {iteration}, skipping...\")\n",
    "        optimizer.zero_grad()\n",
    "        continue\n",
    "    \n",
    "    # Gradient clipping for stability\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Save best model\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            render_mean = rendered.mean().item()\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}', \n",
    "            'view': view_idx,\n",
    "            'render': f'{render_mean:.3f}',\n",
    "            'grad': f'{grad_norm:.2f}'\n",
    "        })\n",
    "\n",
    "# Restore best model if training went bad\n",
    "if losses[-1] > losses[0] * 1.1:  # Final loss > 110% of initial\n",
    "    print(f\"\\n‚ö†Ô∏è Training may have diverged (loss went up). Restoring best model...\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"   Restored to best loss: {best_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization complete!\")\n",
    "print(f\"   Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"   Best loss: {best_loss:.4f}\")\n",
    "print(f\"   Final loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# Test render after optimization\n",
    "with torch.no_grad():\n",
    "    final_render, _ = render_gaussians(model, camera_poses[0], intrinsics, IMAGE_SIZE)\n",
    "    print(f\"   Final render mean: {final_render.mean():.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.axhline(y=best_loss, color='g', linestyle='--', label=f'Best: {best_loss:.4f}')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('gsplat Optimization Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(DIRS['gsplat'] / \"loss_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized model\n",
    "def save_gaussian_ply(model, output_path):\n",
    "    with torch.no_grad():\n",
    "        params = model()\n",
    "        xyz = params['xyz'].cpu().numpy()\n",
    "        colors = model.f_dc.cpu().numpy()\n",
    "        f_rest = model.f_rest.cpu().numpy()\n",
    "        opacity = model.opacity_raw.cpu().numpy()\n",
    "        scales = model.scales_raw.cpu().numpy()\n",
    "        rotations = params['rotations'].cpu().numpy()\n",
    "        \n",
    "    num_points = len(xyz)\n",
    "    dtype_full = [('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "                  ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4')]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        dtype_full.append((f'f_rest_{i}', 'f4'))\n",
    "    dtype_full.extend([('opacity', 'f4'),\n",
    "                       ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),\n",
    "                       ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4')])\n",
    "    \n",
    "    elements = np.zeros(num_points, dtype=dtype_full)\n",
    "    elements['x'] = xyz[:, 0]\n",
    "    elements['y'] = xyz[:, 1]\n",
    "    elements['z'] = xyz[:, 2]\n",
    "    elements['f_dc_0'] = colors[:, 0]\n",
    "    elements['f_dc_1'] = colors[:, 1]\n",
    "    elements['f_dc_2'] = colors[:, 2]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        elements[f'f_rest_{i}'] = f_rest[:, i]\n",
    "    elements['opacity'] = opacity\n",
    "    elements['scale_0'] = scales[:, 0]\n",
    "    elements['scale_1'] = scales[:, 1]\n",
    "    elements['scale_2'] = scales[:, 2]\n",
    "    elements['rot_0'] = rotations[:, 0]\n",
    "    elements['rot_1'] = rotations[:, 1]\n",
    "    elements['rot_2'] = rotations[:, 2]\n",
    "    elements['rot_3'] = rotations[:, 3]\n",
    "    \n",
    "    el = PlyElement.describe(elements, 'vertex')\n",
    "    PlyData([el]).write(output_path)\n",
    "\n",
    "OPTIMIZED_PLY_PATH = DIRS['gsplat'] / \"optimized_gaussian.ply\"\n",
    "save_gaussian_ply(model, str(OPTIMIZED_PLY_PATH))\n",
    "print(f\"‚úÖ Saved optimized Gaussians: {OPTIMIZED_PLY_PATH}\")\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL DIAGNOSTICS: Check if optimization went wrong\n",
    "print(\"\\nüîç Model Parameter Diagnostics:\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    opacity_values = model.opacity.cpu().numpy()\n",
    "    scale_values = model.scales.cpu().numpy()\n",
    "    color_values = model.get_colors().cpu().numpy()\n",
    "    \n",
    "    print(f\"\\nüìä Opacity (sigmoid(opacity_raw)):\")\n",
    "    print(f\"   Range: [{opacity_values.min():.6f}, {opacity_values.max():.6f}]\")\n",
    "    print(f\"   Mean: {opacity_values.mean():.6f}\")\n",
    "    print(f\"   Median: {np.median(opacity_values):.6f}\")\n",
    "    print(f\"   % > 0.5: {(opacity_values > 0.5).sum() / len(opacity_values) * 100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìä Scales (exp(scales_raw)):\")\n",
    "    print(f\"   Range: [{scale_values.min():.6f}, {scale_values.max():.6f}]\")\n",
    "    print(f\"   Mean: {scale_values.mean():.6f}\")\n",
    "    print(f\"   Median: {np.median(scale_values):.6f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Colors (0.5 + C0 * f_dc):\")\n",
    "    print(f\"   Range: [{color_values.min():.6f}, {color_values.max():.6f}]\")\n",
    "    print(f\"   Mean: {color_values.mean():.6f}\")\n",
    "    print(f\"   % gray (< 0.1): {(color_values < 0.1).sum() / color_values.size * 100:.1f}%\")\n",
    "\n",
    "# ‚ö†Ô∏è RECOVERY: If model is completely degenerate, reset to initial\n",
    "if opacity_values.max() < 0.01 or scale_values.max() < 0.001 or color_values.max() < 0.01:\n",
    "    print(\"\\n‚ö†Ô∏è CRITICAL: Model appears to have degenerated during optimization!\")\n",
    "    print(\"   Resetting to initial Gaussians for video rendering...\")\n",
    "    \n",
    "    # Reload initial model\n",
    "    gaussians_initial = load_gaussian_ply(str(INITIAL_PLY_PATH))\n",
    "    model = GaussianModel(gaussians_initial).to(device)\n",
    "    print(\"‚úÖ Model reset to initial state\")\n",
    "    \n",
    "    # Double-check new model\n",
    "    with torch.no_grad():\n",
    "        opacity_check = model.opacity.cpu().numpy()\n",
    "        print(f\"   Opacity check: [{opacity_check.min():.4f}, {opacity_check.max():.4f}]\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Model parameters appear reasonable for rendering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc775e",
   "metadata": {},
   "source": [
    "# Stage 6: Generate Final Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ FINAL OUTPUT GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import imageio\n",
    "\n",
    "# ‚ö†Ô∏è DEBUG: Check actual Gaussian positions to set correct camera\n",
    "with torch.no_grad():\n",
    "    xyz = model.xyz.cpu().numpy()\n",
    "    \n",
    "print(f\"\\nüìä Gaussian Point Cloud Statistics:\")\n",
    "print(f\"   Points: {len(xyz):,}\")\n",
    "print(f\"   X range: [{xyz[:, 0].min():.3f}, {xyz[:, 0].max():.3f}]\")\n",
    "print(f\"   Y range: [{xyz[:, 1].min():.3f}, {xyz[:, 1].max():.3f}]\")\n",
    "print(f\"   Z range: [{xyz[:, 2].min():.3f}, {xyz[:, 2].max():.3f}]\")\n",
    "\n",
    "# Calculate bounding box and center\n",
    "bbox_min = xyz.min(axis=0)\n",
    "bbox_max = xyz.max(axis=0)\n",
    "center = (bbox_min + bbox_max) / 2\n",
    "extent = (bbox_max - bbox_min).max()  # Largest dimension\n",
    "\n",
    "print(f\"   Center: ({center[0]:.3f}, {center[1]:.3f}, {center[2]:.3f})\")\n",
    "print(f\"   Extent: {extent:.3f}\")\n",
    "\n",
    "# ‚úÖ FIX: Auto-calculate camera radius based on actual model size\n",
    "# Camera should be ~2-3x the model extent away\n",
    "AUTO_RADIUS = max(extent * 2.5, 0.5)  # At least 0.5 to avoid being inside model\n",
    "print(f\"   Auto camera radius: {AUTO_RADIUS:.3f}\")\n",
    "\n",
    "# ‚úÖ DEFINE HELPER FUNCTIONS FIRST (before using them)\n",
    "def get_intrinsics_video(fov_deg=49.1, image_size=512):\n",
    "    \"\"\"Get camera intrinsics matrix.\"\"\"\n",
    "    fov_rad = math.radians(fov_deg)\n",
    "    focal = image_size / (2 * math.tan(fov_rad / 2))\n",
    "    K = np.array([\n",
    "        [focal, 0, image_size / 2],\n",
    "        [0, focal, image_size / 2],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "    return K\n",
    "\n",
    "def create_camera_pose_centered(elevation_deg, azimuth_deg, radius, center):\n",
    "    \"\"\"Create world-to-camera matrix looking at a specific center point.\"\"\"\n",
    "    elev = math.radians(elevation_deg)\n",
    "    azim = math.radians(azimuth_deg)\n",
    "    \n",
    "    # Camera position in spherical coordinates around the center (Y-up convention)\n",
    "    x = center[0] + radius * math.cos(elev) * math.sin(azim)\n",
    "    y = center[1] + radius * math.sin(elev)\n",
    "    z = center[2] + radius * math.cos(elev) * math.cos(azim)\n",
    "    \n",
    "    cam_pos = np.array([x, y, z])\n",
    "    look_at = np.array(center)\n",
    "    up = np.array([0, 1, 0])  # Y-up\n",
    "    \n",
    "    # Construct camera basis\n",
    "    forward = look_at - cam_pos\n",
    "    forward_norm = np.linalg.norm(forward)\n",
    "    if forward_norm < 1e-6:\n",
    "        forward = np.array([0, 0, -1])\n",
    "    else:\n",
    "        forward = forward / forward_norm\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right_norm = np.linalg.norm(right)\n",
    "    if right_norm < 1e-6:\n",
    "        # Camera looking straight up/down - use different up vector\n",
    "        up = np.array([0, 0, 1])\n",
    "        right = np.cross(forward, up)\n",
    "        right_norm = np.linalg.norm(right)\n",
    "    right = right / (right_norm + 1e-8)\n",
    "    up_new = np.cross(right, forward)\n",
    "    \n",
    "    # World-to-camera transformation\n",
    "    w2c = np.eye(4, dtype=np.float32)\n",
    "    w2c[0, :3] = right\n",
    "    w2c[1, :3] = up_new\n",
    "    w2c[2, :3] = -forward  # Camera looks along -Z\n",
    "    w2c[:3, 3] = -w2c[:3, :3] @ cam_pos\n",
    "    \n",
    "    return w2c\n",
    "\n",
    "VIDEO_SIZE = 512\n",
    "video_intrinsics = get_intrinsics_video(fov_deg=49.1, image_size=VIDEO_SIZE)\n",
    "\n",
    "# Test render a single frame first to verify it works\n",
    "print(\"\\nüîç Testing single frame render...\")\n",
    "test_w2c = create_camera_pose_centered(20.0, 0.0, AUTO_RADIUS, center)\n",
    "with torch.no_grad():\n",
    "    test_rgb, test_alpha = render_gaussians(model, test_w2c, video_intrinsics, VIDEO_SIZE)\n",
    "    test_rgb_np = test_rgb.cpu().numpy()\n",
    "    \n",
    "print(f\"   Test frame - RGB range: [{test_rgb_np.min():.3f}, {test_rgb_np.max():.3f}]\")\n",
    "print(f\"   Test frame - mean brightness: {test_rgb_np.mean():.3f}\")\n",
    "\n",
    "# ‚ö†Ô∏è ADAPTIVE RADIUS ADJUSTMENT\n",
    "attempt_radius = AUTO_RADIUS\n",
    "max_attempts = 3\n",
    "\n",
    "for attempt in range(max_attempts):\n",
    "    if test_rgb_np.max() < 0.01:\n",
    "        print(f\"   ‚ö†Ô∏è Frame is nearly black (attempt {attempt+1}/{max_attempts})! Trying larger radius...\")\n",
    "        attempt_radius = extent * (3.0 + attempt * 2)\n",
    "        print(f\"   New radius: {attempt_radius:.3f}\")\n",
    "        \n",
    "        test_w2c = create_camera_pose_centered(20.0, 0.0, attempt_radius, center)\n",
    "        with torch.no_grad():\n",
    "            test_rgb, test_alpha = render_gaussians(model, test_w2c, video_intrinsics, VIDEO_SIZE)\n",
    "            test_rgb_np = test_rgb.cpu().numpy()\n",
    "        print(f\"   Retested - RGB range: [{test_rgb_np.min():.3f}, {test_rgb_np.max():.3f}]\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# If still black after all attempts, show diagnostic data\n",
    "if test_rgb_np.max() < 0.01:\n",
    "    print(\"\\nüö® CRITICAL: Still rendering all black after radius adjustment!\")\n",
    "    print(\"\\nüìã Rendering Diagnostics:\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        params = model()\n",
    "        print(f\"   XYZ range: [{params['xyz'].min():.3f}, {params['xyz'].max():.3f}]\")\n",
    "        print(f\"   Colors range: [{params['colors'].min():.3f}, {params['colors'].max():.3f}]\")\n",
    "        print(f\"   Opacity range: [{params['opacity'].min():.6f}, {params['opacity'].max():.6f}]\")\n",
    "        print(f\"   Scales range: [{params['scales'].min():.3f}, {params['scales'].max():.3f}]\")\n",
    "        \n",
    "        # Check if any Gaussians have non-zero parameters\n",
    "        non_black_count = (params['colors'].abs().max(dim=1).values > 0.1).sum().item()\n",
    "        print(f\"   Non-black Gaussians: {non_black_count} / {len(params['xyz'])}\")\n",
    "    \n",
    "    print(\"\\nüí° Possible causes:\")\n",
    "    print(\"   - Optimization failed (opacities went to 0)\")\n",
    "    print(\"   - Gaussian scales became too small (exp of very negative values)\")\n",
    "    print(\"   - Color values all converged to 0.5 (neutral gray)\")\n",
    "    print(\"   - Model was reset to initial state with invalid parameters\")\n",
    "    print(\"\\n   Consider: Re-run gsplat optimization with more iterations\")\n",
    "else:\n",
    "    print(\"‚úÖ Test frame looks good!\")\n",
    "\n",
    "AUTO_RADIUS = attempt_radius\n",
    "\n",
    "print(\"\\nüé¨ Rendering 360¬∞ turntable video...\")\n",
    "video_frames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for azim in tqdm(np.linspace(0, 360, 120, endpoint=False)):\n",
    "        w2c = create_camera_pose_centered(20.0, azim, AUTO_RADIUS, center)\n",
    "        rgb, _ = render_gaussians(model, w2c, video_intrinsics, VIDEO_SIZE)\n",
    "        frame = (rgb.cpu().numpy().clip(0, 1) * 255).astype(np.uint8)\n",
    "        video_frames.append(frame)\n",
    "\n",
    "# Verify frames aren't all black\n",
    "frame_brightness_samples = [np.mean(f) for f in video_frames[::10]]\n",
    "sample_brightness = np.mean(frame_brightness_samples)\n",
    "print(f\"   Average frame brightness: {sample_brightness:.1f}/255\")\n",
    "print(f\"   Min brightness: {min(frame_brightness_samples):.1f}/255\")\n",
    "print(f\"   Max brightness: {max(frame_brightness_samples):.1f}/255\")\n",
    "\n",
    "if sample_brightness < 5:\n",
    "    print(\"   üö® WARNING: Frames appear very dark!\")\n",
    "    print(\"   This might indicate a camera/model mismatch or failed optimization.\")\n",
    "elif sample_brightness < 50:\n",
    "    print(\"   ‚ö†Ô∏è Frames are quite dim - may want to adjust camera settings\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Frames appear properly lit!\")\n",
    "\n",
    "video_path = DIRS['output'] / \"glimpse3d_360.mp4\"\n",
    "imageio.mimsave(str(video_path), video_frames, fps=30)\n",
    "print(f\"‚úÖ Video saved: {video_path}\")\n",
    "\n",
    "# Also save a debug frame\n",
    "debug_frame_path = DIRS['output'] / \"debug_frame_0.png\"\n",
    "Image.fromarray(video_frames[0]).save(debug_frame_path)\n",
    "print(f\"‚úÖ Debug frame saved: {debug_frame_path}\")\n",
    "\n",
    "# Save diagnostic info\n",
    "debug_info_path = DIRS['output'] / \"render_diagnostics.txt\"\n",
    "with open(debug_info_path, 'w') as f:\n",
    "    f.write(\"GLIMPSE3D Rendering Diagnostics\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Gaussian Points: {len(xyz):,}\\n\")\n",
    "    f.write(f\"Center: ({center[0]:.3f}, {center[1]:.3f}, {center[2]:.3f})\\n\")\n",
    "    f.write(f\"Extent: {extent:.3f}\\n\")\n",
    "    f.write(f\"Camera Radius: {AUTO_RADIUS:.3f}\\n\")\n",
    "    f.write(f\"Frame Size: {VIDEO_SIZE}x{VIDEO_SIZE}\\n\")\n",
    "    f.write(f\"Average Brightness: {sample_brightness:.1f}/255\\n\")\n",
    "    f.write(f\"\\nIf video is black: Check that gsplat optimization completed successfully\\n\")\n",
    "print(f\"‚úÖ Diagnostics saved: {debug_info_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdffe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy final files\n",
    "import shutil\n",
    "\n",
    "# Copy optimized PLY\n",
    "final_ply = DIRS['output'] / \"final_gaussian.ply\"\n",
    "shutil.copy(OPTIMIZED_PLY_PATH, final_ply)\n",
    "\n",
    "# Copy mesh\n",
    "shutil.copy(DIRS['triposr'] / \"mesh.glb\", DIRS['output'] / \"initial_mesh.glb\")\n",
    "shutil.copy(DIRS['triposr'] / \"mesh.obj\", DIRS['output'] / \"initial_mesh.obj\")\n",
    "\n",
    "# Copy best views\n",
    "for i in [0, 4, 8, 12]:\n",
    "    shutil.copy(\n",
    "        DIRS['syncdreamer'] / f\"view_{i:02d}_e{int(ELEVATIONS[i])}_a{int(AZIMUTHS[i])}.png\",\n",
    "        DIRS['output'] / f\"view_{i:02d}.png\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüìÅ Final output files:\")\n",
    "for f in sorted(DIRS['output'].iterdir()):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e210fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display video\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(video_path, 'rb').read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "HTML(f'''\n",
    "<h3>üèÜ Glimpse3D Result</h3>\n",
    "<video width=\"600\" controls autoplay loop>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddae42",
   "metadata": {},
   "source": [
    "# üì• Download All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Create final ZIP\n",
    "output_zip = str(WORK_DIR / \"glimpse3d_complete_output\")\n",
    "shutil.make_archive(output_zip, 'zip', DIRS['output'])\n",
    "\n",
    "print(\"üì• Downloading Glimpse3D results...\")\n",
    "files.download(f\"{output_zip}.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ GLIMPSE3D PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDownloaded: glimpse3d_complete_output.zip\")\n",
    "print(\"\\nContents:\")\n",
    "print(\"  - final_gaussian.ply   : Optimized Gaussian Splats\")\n",
    "print(\"  - initial_mesh.glb/obj : TripoSR mesh\")\n",
    "print(\"  - glimpse3d_360.mp4    : 360¬∞ turntable video\")\n",
    "print(\"  - view_*.png           : Multi-view images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ace8d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Pipeline Complete!\n",
    "\n",
    "You now have:\n",
    "1. **final_gaussian.ply** - View in any Gaussian Splat viewer\n",
    "2. **initial_mesh.glb** - View in 3D viewers like Blender, online GLB viewers\n",
    "3. **glimpse3d_360.mp4** - Share as video\n",
    "\n",
    "### Recommended Viewers\n",
    "- **Gaussian Splats**: [SuperSplat](https://playcanvas.com/supersplat/editor), [Luma AI Viewer](https://lumalabs.ai/)\n",
    "- **GLB Mesh**: [glTF Viewer](https://gltf-viewer.donmccurdy.com/), Blender\n",
    "\n",
    "### Tips for Better Results\n",
    "1. Use high-quality input images with clean backgrounds\n",
    "2. Objects should be centered and fill ~80% of the frame\n",
    "3. Avoid reflective or transparent surfaces\n",
    "4. Run more gsplat iterations (2000+) for higher quality"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
