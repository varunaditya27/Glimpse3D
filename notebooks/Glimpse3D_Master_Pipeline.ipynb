{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b3e518",
   "metadata": {},
   "source": [
    "# üéØ Glimpse3D - Master Pipeline\n",
    "\n",
    "## Complete 2D Image ‚Üí 3D Gaussian Splat Pipeline\n",
    "\n",
    "This notebook runs the **entire Glimpse3D pipeline** end-to-end:\n",
    "\n",
    "```\n",
    "üì∑ Input Image\n",
    "    ‚Üì\n",
    "üî∑ TripoSR (0.5s) ‚Üí Initial 3D Mesh ‚Üí Gaussian Points\n",
    "    ‚Üì\n",
    "üé® SyncDreamer (2min) ‚Üí 16 Consistent Multi-View Images\n",
    "    ‚Üì  \n",
    "‚ú® SDXL Lightning + ControlNet ‚Üí Enhanced Views\n",
    "    ‚Üì\n",
    "üîÆ gsplat Optimization ‚Üí Refined Gaussians\n",
    "    ‚Üì\n",
    "üîÑ MVCRM ‚Üí Multi-View Consistent Refinement\n",
    "    ‚Üì\n",
    "üèÜ Final 3D Gaussian Splat Output\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with **T4 GPU** (free tier) or **A100** (faster)\n",
    "- ~12GB VRAM peak usage\n",
    "- ~30 minutes total runtime\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b9736",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start\n",
    "\n",
    "1. Run all cells in order (Runtime ‚Üí Run all)\n",
    "2. Upload your image when prompted\n",
    "3. Wait ~30 minutes for full pipeline\n",
    "4. Download final results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75e6c",
   "metadata": {},
   "source": [
    "# Stage 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42332bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"üñ•Ô∏è Running in Colab: {IN_COLAB}\")\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüì¶ PyTorch: {torch.__version__}\")\n",
    "print(f\"üî• CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_VRAM = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"üéÆ GPU: {GPU_NAME}\")\n",
    "    print(f\"üíæ VRAM: {GPU_VRAM:.1f} GB\")\n",
    "    \n",
    "    # Set batch sizes based on GPU\n",
    "    if GPU_VRAM >= 40:  # A100\n",
    "        BATCH_VIEW_NUM = 8\n",
    "        MC_RESOLUTION = 384\n",
    "        NUM_SAMPLES = 200000\n",
    "    elif GPU_VRAM >= 15:  # T4\n",
    "        BATCH_VIEW_NUM = 4\n",
    "        MC_RESOLUTION = 256\n",
    "        NUM_SAMPLES = 100000\n",
    "    else:\n",
    "        BATCH_VIEW_NUM = 2\n",
    "        MC_RESOLUTION = 192\n",
    "        NUM_SAMPLES = 50000\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Settings: batch_view={BATCH_VIEW_NUM}, resolution={MC_RESOLUTION}, samples={NUM_SAMPLES}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå No GPU available! Enable GPU in Runtime ‚Üí Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "# Install all dependencies\n",
    "print(\"üì¶ Installing dependencies (this takes ~5 minutes)...\")\n",
    "\n",
    "# Core packages\n",
    "!pip install torch torchvision --quiet\n",
    "!pip install transformers diffusers accelerate huggingface_hub --quiet\n",
    "!pip install omegaconf einops pytorch-lightning==1.9.0 kornia --quiet\n",
    "\n",
    "# TripoSR dependencies\n",
    "!pip install trimesh rembg[gpu] xatlas plyfile --quiet\n",
    "!pip install git+https://github.com/tatsy/torchmcubes.git --quiet\n",
    "\n",
    "# gsplat\n",
    "!pip install gsplat --quiet\n",
    "\n",
    "# SyncDreamer dependencies\n",
    "!pip install git+https://github.com/openai/CLIP.git --quiet\n",
    "!pip install taming-transformers-rom1504 --quiet\n",
    "\n",
    "# Image processing\n",
    "!pip install opencv-python-headless scikit-image imageio --quiet\n",
    "\n",
    "# Depth estimation\n",
    "!pip install timm --quiet\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f719d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "WORK_DIR = Path(\"/content/glimpse3d_pipeline\")\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DIRS = {\n",
    "    'input': WORK_DIR / 'input',\n",
    "    'triposr': WORK_DIR / 'stage1_triposr',\n",
    "    'syncdreamer': WORK_DIR / 'stage2_syncdreamer',\n",
    "    'enhanced': WORK_DIR / 'stage3_enhanced',\n",
    "    'gsplat': WORK_DIR / 'stage4_gsplat',\n",
    "    'mvcrm': WORK_DIR / 'stage5_mvcrm',\n",
    "    'output': WORK_DIR / 'final_output',\n",
    "}\n",
    "\n",
    "for name, path in DIRS.items():\n",
    "    path.mkdir(exist_ok=True)\n",
    "    print(f\"üìÅ {name}: {path}\")\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Clear GPU memory between stages.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"üßπ GPU memory cleared. Using: {allocated:.2f} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ Directory structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a932c3f",
   "metadata": {},
   "source": [
    "# Stage 1: Upload Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"üì§ Upload your image (JPG/PNG):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Save uploaded file\n",
    "INPUT_FILENAME = list(uploaded.keys())[0]\n",
    "INPUT_PATH = DIRS['input'] / INPUT_FILENAME\n",
    "\n",
    "with open(INPUT_PATH, 'wb') as f:\n",
    "    f.write(list(uploaded.values())[0])\n",
    "\n",
    "# Display\n",
    "input_image = Image.open(INPUT_PATH)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(input_image)\n",
    "plt.title(f\"Input: {INPUT_FILENAME} ({input_image.size[0]}x{input_image.size[1]})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Saved to: {INPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec82a1a",
   "metadata": {},
   "source": [
    "# Stage 2: TripoSR - Initial 3D Reconstruction\n",
    "\n",
    "**Input:** Single image  \n",
    "**Output:** 3D mesh + Gaussian point cloud  \n",
    "**Time:** ~30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0772717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone TripoSR\n",
    "TRIPOSR_PATH = Path(\"/content/TripoSR\")\n",
    "\n",
    "if not TRIPOSR_PATH.exists():\n",
    "    print(\"üì• Cloning TripoSR...\")\n",
    "    !git clone https://github.com/VAST-AI-Research/TripoSR.git {TRIPOSR_PATH}\n",
    "\n",
    "sys.path.insert(0, str(TRIPOSR_PATH))\n",
    "os.chdir(TRIPOSR_PATH)\n",
    "print(f\"‚úÖ TripoSR ready at {TRIPOSR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tsr.system import TSR\n",
    "from tsr.utils import remove_background, resize_foreground\n",
    "import rembg\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî∑ STAGE 2: TripoSR 3D Reconstruction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Load model\n",
    "print(\"\\nüì• Loading TripoSR model...\")\n",
    "triposr_model = TSR.from_pretrained(\n",
    "    \"stabilityai/TripoSR\",\n",
    "    config_name=\"config.yaml\",\n",
    "    weight_name=\"model.ckpt\",\n",
    ")\n",
    "triposr_model.renderer.set_chunk_size(8192)\n",
    "triposr_model.to(device)\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# Preprocess image\n",
    "print(\"\\nüîß Preprocessing image...\")\n",
    "input_img = Image.open(INPUT_PATH)\n",
    "rembg_session = rembg.new_session()\n",
    "processed_img = remove_background(input_img, rembg_session)\n",
    "processed_img = resize_foreground(processed_img, 0.85)\n",
    "\n",
    "# Convert to RGB\n",
    "img_np = np.array(processed_img).astype(np.float32) / 255.0\n",
    "img_np = img_np[:, :, :3] * img_np[:, :, 3:4] + (1 - img_np[:, :, 3:4]) * 0.5\n",
    "processed_img = Image.fromarray((img_np * 255.0).astype(np.uint8))\n",
    "processed_img.save(DIRS['triposr'] / \"processed_input.png\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\nüöÄ Running TripoSR...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    scene_codes = triposr_model([processed_img], device=device)\n",
    "    meshes = triposr_model.extract_mesh(scene_codes, has_vertex_color=True, resolution=MC_RESOLUTION)\n",
    "\n",
    "mesh = meshes[0]\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Mesh generated in {elapsed:.2f}s\")\n",
    "print(f\"   Vertices: {len(mesh.vertices):,}\")\n",
    "print(f\"   Faces: {len(mesh.faces):,}\")\n",
    "\n",
    "# Save mesh\n",
    "mesh.export(str(DIRS['triposr'] / \"mesh.obj\"))\n",
    "mesh.export(str(DIRS['triposr'] / \"mesh.glb\"))\n",
    "print(f\"\\nüìÅ Saved mesh to {DIRS['triposr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70122c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mesh to Gaussian PLY\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def mesh_to_gaussian_ply(mesh, output_path, num_samples=100000):\n",
    "    \"\"\"Convert mesh to Gaussian Splat format.\"\"\"\n",
    "    print(f\"\\nüîÑ Sampling {num_samples:,} points...\")\n",
    "    \n",
    "    points, face_indices = mesh.sample(num_samples, return_index=True)\n",
    "    \n",
    "    if mesh.visual.vertex_colors is not None:\n",
    "        face_vertices = mesh.faces[face_indices]\n",
    "        vertex_colors = mesh.visual.vertex_colors[:, :3] / 255.0\n",
    "        colors = vertex_colors[face_vertices].mean(axis=1)\n",
    "    else:\n",
    "        colors = np.ones((num_samples, 3)) * 0.5\n",
    "    \n",
    "    num_points = len(points)\n",
    "    xyz = points.astype(np.float32)\n",
    "    \n",
    "    C0 = 0.28209479177387814\n",
    "    features_dc = ((colors - 0.5) / C0).astype(np.float32)\n",
    "    features_rest = np.zeros((num_points, 45), dtype=np.float32)\n",
    "    opacities = np.ones((num_points, 1), dtype=np.float32) * 2.2\n",
    "    scales = np.ones((num_points, 3), dtype=np.float32) * (-4.6)\n",
    "    rotations = np.zeros((num_points, 4), dtype=np.float32)\n",
    "    rotations[:, 0] = 1.0\n",
    "    \n",
    "    dtype_full = [\n",
    "        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "        ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4'),\n",
    "    ]\n",
    "    for i in range(45):\n",
    "        dtype_full.append((f'f_rest_{i}', 'f4'))\n",
    "    dtype_full.extend([\n",
    "        ('opacity', 'f4'),\n",
    "        ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),\n",
    "        ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4'),\n",
    "    ])\n",
    "    \n",
    "    elements = np.zeros(num_points, dtype=dtype_full)\n",
    "    elements['x'] = xyz[:, 0]\n",
    "    elements['y'] = xyz[:, 1]\n",
    "    elements['z'] = xyz[:, 2]\n",
    "    elements['f_dc_0'] = features_dc[:, 0]\n",
    "    elements['f_dc_1'] = features_dc[:, 1]\n",
    "    elements['f_dc_2'] = features_dc[:, 2]\n",
    "    for i in range(45):\n",
    "        elements[f'f_rest_{i}'] = features_rest[:, i]\n",
    "    elements['opacity'] = opacities[:, 0]\n",
    "    elements['scale_0'] = scales[:, 0]\n",
    "    elements['scale_1'] = scales[:, 1]\n",
    "    elements['scale_2'] = scales[:, 2]\n",
    "    elements['rot_0'] = rotations[:, 0]\n",
    "    elements['rot_1'] = rotations[:, 1]\n",
    "    elements['rot_2'] = rotations[:, 2]\n",
    "    elements['rot_3'] = rotations[:, 3]\n",
    "    \n",
    "    el = PlyElement.describe(elements, 'vertex')\n",
    "    PlyData([el]).write(output_path)\n",
    "    print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "INITIAL_PLY_PATH = DIRS['triposr'] / \"initial_gaussian.ply\"\n",
    "mesh_to_gaussian_ply(mesh, str(INITIAL_PLY_PATH), num_samples=NUM_SAMPLES)\n",
    "\n",
    "# Cleanup TripoSR\n",
    "del triposr_model, mesh, scene_codes\n",
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fc9c3",
   "metadata": {},
   "source": [
    "# Stage 3: SyncDreamer - Multi-View Generation\n",
    "\n",
    "**Input:** Processed image  \n",
    "**Output:** 16 consistent multi-view images  \n",
    "**Time:** ~2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17879096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üé® STAGE 3: SyncDreamer Multi-View Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clone SyncDreamer\n",
    "SYNCDREAMER_PATH = Path(\"/content/SyncDreamer\")\n",
    "\n",
    "if not SYNCDREAMER_PATH.exists():\n",
    "    print(\"üì• Cloning SyncDreamer...\")\n",
    "    !git clone https://github.com/liuyuan-pal/SyncDreamer.git {SYNCDREAMER_PATH}\n",
    "\n",
    "# Download checkpoints\n",
    "CKPT_DIR = SYNCDREAMER_PATH / \"ckpt\"\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "!apt -y install -qq aria2\n",
    "\n",
    "CHECKPOINTS = {\n",
    "    \"syncdreamer-pretrain.ckpt\": \"https://huggingface.co/camenduru/SyncDreamer/resolve/main/syncdreamer-pretrain.ckpt\",\n",
    "    \"ViT-L-14.pt\": \"https://huggingface.co/camenduru/SyncDreamer/resolve/main/ViT-L-14.pt\"\n",
    "}\n",
    "\n",
    "for fname, url in CHECKPOINTS.items():\n",
    "    fpath = CKPT_DIR / fname\n",
    "    if not fpath.exists():\n",
    "        print(f\"üì• Downloading {fname}...\")\n",
    "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}\" -d \"{CKPT_DIR}\" -o \"{fname}\"\n",
    "    else:\n",
    "        print(f\"‚úÖ {fname} exists\")\n",
    "\n",
    "sys.path.insert(0, str(SYNCDREAMER_PATH))\n",
    "os.chdir(SYNCDREAMER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "# Load SyncDreamer model\n",
    "print(\"\\nüì• Loading SyncDreamer model...\")\n",
    "\n",
    "config = OmegaConf.load(SYNCDREAMER_PATH / \"configs\" / \"syncdreamer.yaml\")\n",
    "syncdreamer_model = instantiate_from_config(config.model)\n",
    "\n",
    "state_dict = torch.load(CKPT_DIR / \"syncdreamer-pretrain.ckpt\", map_location=\"cpu\")[\"state_dict\"]\n",
    "syncdreamer_model.load_state_dict(state_dict, strict=True)\n",
    "syncdreamer_model = syncdreamer_model.cuda().eval()\n",
    "\n",
    "print(\"‚úÖ SyncDreamer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.models.diffusion.sync_dreamer import SyncDDIMSampler\n",
    "import clip\n",
    "\n",
    "# Prepare CLIP encoder\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-L/14\", device=\"cuda\")\n",
    "\n",
    "# Prepare input image for SyncDreamer\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load processed image\n",
    "processed_path = DIRS['triposr'] / \"processed_input.png\"\n",
    "input_img = Image.open(processed_path).convert('RGB')\n",
    "input_img = input_img.resize((256, 256))\n",
    "\n",
    "# Convert to tensor\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "input_tensor = transform(input_img).unsqueeze(0).cuda()\n",
    "\n",
    "# CLIP embedding\n",
    "clip_input = clip_preprocess(input_img).unsqueeze(0).cuda()\n",
    "with torch.no_grad():\n",
    "    clip_emb = clip_model.encode_image(clip_input)\n",
    "    clip_emb = clip_emb / clip_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"‚úÖ Input prepared for SyncDreamer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2618cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SyncDreamer inference\n",
    "print(\"\\nüöÄ Running SyncDreamer (this takes ~2-3 minutes)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Settings\n",
    "ELEVATION = 30.0\n",
    "SAMPLE_STEPS = 50\n",
    "CFG_SCALE = 2.0\n",
    "\n",
    "sampler = SyncDDIMSampler(syncdreamer_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Prepare conditioning\n",
    "    cond = {\n",
    "        'input_image': input_tensor,\n",
    "        'clip_emb': clip_emb,\n",
    "        'elevation': torch.tensor([ELEVATION]).cuda(),\n",
    "    }\n",
    "    \n",
    "    # Sample\n",
    "    samples = sampler.sample(\n",
    "        syncdreamer_model,\n",
    "        cond,\n",
    "        batch_view_num=BATCH_VIEW_NUM,\n",
    "        ddim_steps=SAMPLE_STEPS,\n",
    "        unconditional_guidance_scale=CFG_SCALE,\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ SyncDreamer completed in {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d504fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save multi-view images\n",
    "print(\"\\nüíæ Saving multi-view images...\")\n",
    "\n",
    "# Convert samples to images\n",
    "samples = (samples + 1) / 2  # [-1,1] -> [0,1]\n",
    "samples = samples.clamp(0, 1)\n",
    "\n",
    "syncdreamer_views = []\n",
    "ELEVATIONS = [30.0] * 8 + [-20.0] * 8\n",
    "AZIMUTHS = [i * 45.0 for i in range(8)] * 2\n",
    "\n",
    "for i in range(16):\n",
    "    img_tensor = samples[0, i]  # (C, H, W)\n",
    "    img_np = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    img_pil = Image.fromarray(img_np)\n",
    "    \n",
    "    # Save\n",
    "    save_path = DIRS['syncdreamer'] / f\"view_{i:02d}_e{int(ELEVATIONS[i])}_a{int(AZIMUTHS[i])}.png\"\n",
    "    img_pil.save(save_path)\n",
    "    syncdreamer_views.append(img_pil)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(syncdreamer_views)} views to {DIRS['syncdreamer']}\")\n",
    "\n",
    "# Display grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(syncdreamer_views[i])\n",
    "    ax.set_title(f\"E={ELEVATIONS[i]}¬∞ A={AZIMUTHS[i]}¬∞\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"SyncDreamer: 16 Multi-View Images\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIRS['syncdreamer'] / \"grid.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Cleanup SyncDreamer\n",
    "del syncdreamer_model, sampler, samples, clip_model\n",
    "clear_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456650c8",
   "metadata": {},
   "source": [
    "# Stage 4: SDXL Enhancement (Optional)\n",
    "\n",
    "**Input:** Multi-view images  \n",
    "**Output:** Enhanced multi-view images  \n",
    "**Time:** ~1 minute per image\n",
    "\n",
    "Skip this stage if you want faster results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_ENHANCEMENT = False  # Set to True to skip this stage\n",
    "\n",
    "if not SKIP_ENHANCEMENT:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ú® STAGE 4: SDXL Lightning Enhancement\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    from diffusers import StableDiffusionXLImg2ImgPipeline, AutoencoderKL\n",
    "    \n",
    "    # Load SDXL Lightning\n",
    "    print(\"\\nüì• Loading SDXL Lightning...\")\n",
    "    \n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        \"madebyollin/sdxl-vae-fp16-fix\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        vae=vae,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Load Lightning LoRA\n",
    "    pipe.load_lora_weights(\"ByteDance/SDXL-Lightning\", weight_name=\"sdxl_lightning_4step_lora.safetensors\")\n",
    "    pipe.fuse_lora()\n",
    "    \n",
    "    print(\"‚úÖ SDXL Lightning loaded!\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping enhancement stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_ENHANCEMENT:\n",
    "    # Enhance select views (not all 16 to save time)\n",
    "    VIEWS_TO_ENHANCE = [0, 2, 4, 6, 8, 10, 12, 14]  # Every other view\n",
    "    \n",
    "    print(f\"\\nüöÄ Enhancing {len(VIEWS_TO_ENHANCE)} views...\")\n",
    "    \n",
    "    enhanced_views = syncdreamer_views.copy()  # Start with original\n",
    "    \n",
    "    prompt = \"highly detailed 3D render, professional lighting, sharp textures, 8k quality\"\n",
    "    negative_prompt = \"blurry, low quality, artifacts, noise\"\n",
    "    \n",
    "    for i, view_idx in enumerate(VIEWS_TO_ENHANCE):\n",
    "        print(f\"  Enhancing view {view_idx} ({i+1}/{len(VIEWS_TO_ENHANCE)})...\")\n",
    "        \n",
    "        input_img = syncdreamer_views[view_idx].resize((512, 512))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = pipe(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=input_img,\n",
    "                strength=0.3,\n",
    "                num_inference_steps=4,\n",
    "                guidance_scale=0,\n",
    "            ).images[0]\n",
    "        \n",
    "        enhanced_views[view_idx] = result\n",
    "        result.save(DIRS['enhanced'] / f\"enhanced_{view_idx:02d}.png\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Enhanced views saved to {DIRS['enhanced']}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del pipe, vae\n",
    "    clear_gpu()\n",
    "else:\n",
    "    enhanced_views = syncdreamer_views\n",
    "    print(\"Using original SyncDreamer views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cacdbd",
   "metadata": {},
   "source": [
    "# Stage 5: gsplat Optimization\n",
    "\n",
    "**Input:** Initial Gaussian PLY + Multi-view images  \n",
    "**Output:** Optimized Gaussian Splats  \n",
    "**Time:** ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÆ STAGE 5: gsplat Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import torch.nn as nn\n",
    "from gsplat import rasterization\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Load initial Gaussians\n",
    "def load_gaussian_ply(path):\n",
    "    plydata = PlyData.read(path)\n",
    "    vertex = plydata['vertex']\n",
    "    \n",
    "    xyz = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=-1)\n",
    "    f_dc = np.stack([vertex['f_dc_0'], vertex['f_dc_1'], vertex['f_dc_2']], axis=-1)\n",
    "    f_rest_names = [f'f_rest_{i}' for i in range(45)]\n",
    "    f_rest = np.stack([vertex[name] for name in f_rest_names if name in vertex.data.dtype.names], axis=-1)\n",
    "    opacity = vertex['opacity']\n",
    "    scales = np.stack([vertex['scale_0'], vertex['scale_1'], vertex['scale_2']], axis=-1)\n",
    "    rotations = np.stack([vertex['rot_0'], vertex['rot_1'], vertex['rot_2'], vertex['rot_3']], axis=-1)\n",
    "    \n",
    "    return {\n",
    "        'xyz': torch.tensor(xyz, dtype=torch.float32),\n",
    "        'f_dc': torch.tensor(f_dc, dtype=torch.float32),\n",
    "        'f_rest': torch.tensor(f_rest, dtype=torch.float32),\n",
    "        'opacity': torch.tensor(opacity, dtype=torch.float32),\n",
    "        'scales': torch.tensor(scales, dtype=torch.float32),\n",
    "        'rotations': torch.tensor(rotations, dtype=torch.float32),\n",
    "    }\n",
    "\n",
    "gaussians = load_gaussian_ply(str(INITIAL_PLY_PATH))\n",
    "print(f\"‚úÖ Loaded {len(gaussians['xyz']):,} Gaussians\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianModel(nn.Module):\n",
    "    def __init__(self, gaussians):\n",
    "        super().__init__()\n",
    "        self.xyz = nn.Parameter(gaussians['xyz'].clone())\n",
    "        self.f_dc = nn.Parameter(gaussians['f_dc'].clone())\n",
    "        self.f_rest = nn.Parameter(gaussians['f_rest'].clone())\n",
    "        self.opacity_raw = nn.Parameter(gaussians['opacity'].clone())\n",
    "        self.scales_raw = nn.Parameter(gaussians['scales'].clone())\n",
    "        self.rotations = nn.Parameter(gaussians['rotations'].clone())\n",
    "        \n",
    "    @property\n",
    "    def opacity(self):\n",
    "        return torch.sigmoid(self.opacity_raw)\n",
    "    \n",
    "    @property\n",
    "    def scales(self):\n",
    "        return torch.exp(self.scales_raw)\n",
    "    \n",
    "    def get_colors(self):\n",
    "        C0 = 0.28209479177387814\n",
    "        return 0.5 + C0 * self.f_dc\n",
    "    \n",
    "    def forward(self):\n",
    "        return {\n",
    "            'xyz': self.xyz,\n",
    "            'colors': self.get_colors(),\n",
    "            'opacity': self.opacity,\n",
    "            'scales': self.scales,\n",
    "            'rotations': self.rotations / (self.rotations.norm(dim=-1, keepdim=True) + 1e-8),\n",
    "        }\n",
    "\n",
    "model = GaussianModel(gaussians).to(device)\n",
    "print(f\"‚úÖ Model: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera system matching SyncDreamer\n",
    "def create_camera_pose(elevation_deg, azimuth_deg, radius=2.0):\n",
    "    elev = math.radians(elevation_deg)\n",
    "    azim = math.radians(azimuth_deg)\n",
    "    \n",
    "    x = radius * math.cos(elev) * math.cos(azim)\n",
    "    y = radius * math.cos(elev) * math.sin(azim)\n",
    "    z = radius * math.sin(elev)\n",
    "    \n",
    "    cam_pos = np.array([x, y, z])\n",
    "    look_at = np.array([0, 0, 0])\n",
    "    up = np.array([0, 0, 1])\n",
    "    \n",
    "    forward = look_at - cam_pos\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    up_new = np.cross(right, forward)\n",
    "    \n",
    "    w2c = np.eye(4)\n",
    "    w2c[:3, 0] = right\n",
    "    w2c[:3, 1] = up_new\n",
    "    w2c[:3, 2] = -forward\n",
    "    w2c[:3, 3] = -w2c[:3, :3] @ cam_pos\n",
    "    return w2c\n",
    "\n",
    "def get_projection_matrix(fov_deg=60, aspect=1.0):\n",
    "    fov_rad = math.radians(fov_deg)\n",
    "    f = 1.0 / math.tan(fov_rad / 2)\n",
    "    proj = np.zeros((4, 4))\n",
    "    proj[0, 0] = f / aspect\n",
    "    proj[1, 1] = f\n",
    "    proj[2, 2] = -1.01\n",
    "    proj[2, 3] = -0.2\n",
    "    proj[3, 2] = -1\n",
    "    return proj\n",
    "\n",
    "camera_poses = [create_camera_pose(e, a) for e, a in zip(ELEVATIONS, AZIMUTHS)]\n",
    "projection = get_projection_matrix()\n",
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a425de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_gaussians(model, w2c, proj, image_size):\n",
    "    params = model()\n",
    "    \n",
    "    viewmat = torch.tensor(w2c, dtype=torch.float32, device=device)\n",
    "    K = torch.tensor([\n",
    "        [proj[0, 0] * image_size / 2, 0, image_size / 2],\n",
    "        [0, proj[1, 1] * image_size / 2, image_size / 2],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=torch.float32, device=device)\n",
    "    \n",
    "    render_colors, render_alphas, _ = rasterization(\n",
    "        means=params['xyz'],\n",
    "        quats=params['rotations'],\n",
    "        scales=params['scales'],\n",
    "        opacities=params['opacity'],\n",
    "        colors=params['colors'],\n",
    "        viewmats=viewmat.unsqueeze(0),\n",
    "        Ks=K.unsqueeze(0),\n",
    "        width=image_size,\n",
    "        height=image_size,\n",
    "        packed=False,\n",
    "        render_mode=\"RGB\",\n",
    "    )\n",
    "    \n",
    "    return render_colors[0], render_alphas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38273f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Prepare target images\n",
    "target_tensors = []\n",
    "for img in enhanced_views:\n",
    "    img_resized = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_tensor = torch.tensor(np.array(img_resized) / 255.0, dtype=torch.float32, device=device)\n",
    "    target_tensors.append(img_tensor)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.xyz, 'lr': 1e-4},\n",
    "    {'params': model.f_dc, 'lr': 1e-3},\n",
    "    {'params': model.f_rest, 'lr': 1e-3 / 20},\n",
    "    {'params': model.opacity_raw, 'lr': 0.05},\n",
    "    {'params': model.scales_raw, 'lr': 5e-3},\n",
    "    {'params': model.rotations, 'lr': 1e-3},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "\n",
    "# Training\n",
    "NUM_ITERATIONS = 1000\n",
    "losses = []\n",
    "\n",
    "print(\"\\nüöÄ Starting optimization...\")\n",
    "pbar = tqdm(range(NUM_ITERATIONS))\n",
    "\n",
    "for iteration in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample random view\n",
    "    view_idx = np.random.randint(0, 16)\n",
    "    w2c = camera_poses[view_idx]\n",
    "    target = target_tensors[view_idx]\n",
    "    \n",
    "    rendered, alpha = render_gaussians(model, w2c, projection, IMAGE_SIZE)\n",
    "    \n",
    "    loss = F.mse_loss(rendered, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization complete! Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized model\n",
    "def save_gaussian_ply(model, output_path):\n",
    "    with torch.no_grad():\n",
    "        params = model()\n",
    "        xyz = params['xyz'].cpu().numpy()\n",
    "        colors = model.f_dc.cpu().numpy()\n",
    "        f_rest = model.f_rest.cpu().numpy()\n",
    "        opacity = model.opacity_raw.cpu().numpy()\n",
    "        scales = model.scales_raw.cpu().numpy()\n",
    "        rotations = params['rotations'].cpu().numpy()\n",
    "        \n",
    "    num_points = len(xyz)\n",
    "    dtype_full = [('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "                  ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4')]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        dtype_full.append((f'f_rest_{i}', 'f4'))\n",
    "    dtype_full.extend([('opacity', 'f4'),\n",
    "                       ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),\n",
    "                       ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4')])\n",
    "    \n",
    "    elements = np.zeros(num_points, dtype=dtype_full)\n",
    "    elements['x'] = xyz[:, 0]\n",
    "    elements['y'] = xyz[:, 1]\n",
    "    elements['z'] = xyz[:, 2]\n",
    "    elements['f_dc_0'] = colors[:, 0]\n",
    "    elements['f_dc_1'] = colors[:, 1]\n",
    "    elements['f_dc_2'] = colors[:, 2]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        elements[f'f_rest_{i}'] = f_rest[:, i]\n",
    "    elements['opacity'] = opacity\n",
    "    elements['scale_0'] = scales[:, 0]\n",
    "    elements['scale_1'] = scales[:, 1]\n",
    "    elements['scale_2'] = scales[:, 2]\n",
    "    elements['rot_0'] = rotations[:, 0]\n",
    "    elements['rot_1'] = rotations[:, 1]\n",
    "    elements['rot_2'] = rotations[:, 2]\n",
    "    elements['rot_3'] = rotations[:, 3]\n",
    "    \n",
    "    el = PlyElement.describe(elements, 'vertex')\n",
    "    PlyData([el]).write(output_path)\n",
    "\n",
    "OPTIMIZED_PLY_PATH = DIRS['gsplat'] / \"optimized_gaussian.ply\"\n",
    "save_gaussian_ply(model, str(OPTIMIZED_PLY_PATH))\n",
    "print(f\"‚úÖ Saved optimized Gaussians: {OPTIMIZED_PLY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc775e",
   "metadata": {},
   "source": [
    "# Stage 6: Generate Final Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ FINAL OUTPUT GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import imageio\n",
    "\n",
    "# Generate 360¬∞ video\n",
    "print(\"\\nüé¨ Rendering 360¬∞ turntable video...\")\n",
    "video_frames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for azim in tqdm(np.linspace(0, 360, 120)):\n",
    "        w2c = create_camera_pose(30.0, azim, radius=2.0)\n",
    "        rgb, _ = render_gaussians(model, w2c, projection, 512)\n",
    "        frame = (rgb.cpu().numpy().clip(0, 1) * 255).astype(np.uint8)\n",
    "        video_frames.append(frame)\n",
    "\n",
    "video_path = DIRS['output'] / \"glimpse3d_360.mp4\"\n",
    "imageio.mimsave(str(video_path), video_frames, fps=30)\n",
    "print(f\"‚úÖ Video saved: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdffe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy final files\n",
    "import shutil\n",
    "\n",
    "# Copy optimized PLY\n",
    "final_ply = DIRS['output'] / \"final_gaussian.ply\"\n",
    "shutil.copy(OPTIMIZED_PLY_PATH, final_ply)\n",
    "\n",
    "# Copy mesh\n",
    "shutil.copy(DIRS['triposr'] / \"mesh.glb\", DIRS['output'] / \"initial_mesh.glb\")\n",
    "shutil.copy(DIRS['triposr'] / \"mesh.obj\", DIRS['output'] / \"initial_mesh.obj\")\n",
    "\n",
    "# Copy best views\n",
    "for i in [0, 4, 8, 12]:\n",
    "    shutil.copy(\n",
    "        DIRS['syncdreamer'] / f\"view_{i:02d}_e{int(ELEVATIONS[i])}_a{int(AZIMUTHS[i])}.png\",\n",
    "        DIRS['output'] / f\"view_{i:02d}.png\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüìÅ Final output files:\")\n",
    "for f in sorted(DIRS['output'].iterdir()):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e210fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display video\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(video_path, 'rb').read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "HTML(f'''\n",
    "<h3>üèÜ Glimpse3D Result</h3>\n",
    "<video width=\"600\" controls autoplay loop>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddae42",
   "metadata": {},
   "source": [
    "# üì• Download All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Create final ZIP\n",
    "output_zip = str(WORK_DIR / \"glimpse3d_complete_output\")\n",
    "shutil.make_archive(output_zip, 'zip', DIRS['output'])\n",
    "\n",
    "print(\"üì• Downloading Glimpse3D results...\")\n",
    "files.download(f\"{output_zip}.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ GLIMPSE3D PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDownloaded: glimpse3d_complete_output.zip\")\n",
    "print(\"\\nContents:\")\n",
    "print(\"  - final_gaussian.ply   : Optimized Gaussian Splats\")\n",
    "print(\"  - initial_mesh.glb/obj : TripoSR mesh\")\n",
    "print(\"  - glimpse3d_360.mp4    : 360¬∞ turntable video\")\n",
    "print(\"  - view_*.png           : Multi-view images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ace8d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Pipeline Complete!\n",
    "\n",
    "You now have:\n",
    "1. **final_gaussian.ply** - View in any Gaussian Splat viewer\n",
    "2. **initial_mesh.glb** - View in 3D viewers like Blender, online GLB viewers\n",
    "3. **glimpse3d_360.mp4** - Share as video\n",
    "\n",
    "### Recommended Viewers\n",
    "- **Gaussian Splats**: [SuperSplat](https://playcanvas.com/supersplat/editor), [Luma AI Viewer](https://lumalabs.ai/)\n",
    "- **GLB Mesh**: [glTF Viewer](https://gltf-viewer.donmccurdy.com/), Blender\n",
    "\n",
    "### Tips for Better Results\n",
    "1. Use high-quality input images with clean backgrounds\n",
    "2. Objects should be centered and fill ~80% of the frame\n",
    "3. Avoid reflective or transparent surfaces\n",
    "4. Run more gsplat iterations (2000+) for higher quality"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
