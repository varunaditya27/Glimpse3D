{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480fcaf9",
   "metadata": {},
   "source": [
    "# üéØ SyncDreamer Inference on Google Colab (T4 GPU)\n",
    "\n",
    "**Generate 16 Multi-View Consistent Images from a Single Image**\n",
    "\n",
    "This notebook runs SyncDreamer inference on a T4 GPU (15GB VRAM) with optimized settings.\n",
    "\n",
    "## What this notebook does:\n",
    "1. ‚úÖ Clones SyncDreamer repository\n",
    "2. ‚úÖ Installs all dependencies\n",
    "3. ‚úÖ Downloads pretrained checkpoints (~6GB total)\n",
    "4. ‚úÖ Configures memory-efficient settings for T4\n",
    "5. ‚úÖ Runs inference on a test image\n",
    "6. ‚úÖ Displays 16 generated views in a grid\n",
    "\n",
    "**‚ö†Ô∏è Make sure to select GPU runtime: Runtime ‚Üí Change runtime type ‚Üí T4 GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edbbc89",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Clone SyncDreamer repository\n",
    "%cd /content\n",
    "!git clone https://github.com/liuyuan-pal/SyncDreamer.git\n",
    "%cd /content/SyncDreamer\n",
    "\n",
    "# Create checkpoint directory\n",
    "!mkdir -p ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff09c3",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies\n",
    "\n",
    "This installs all required packages. May take 2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d33042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab already has PyTorch pre-installed - just use it!\n",
    "# Only install if needed (uncomment if you get version issues):\n",
    "# !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q omegaconf pytorch-lightning==1.9.0 einops kornia\n",
    "!pip install -q transformers diffusers accelerate\n",
    "\n",
    "# Install CLIP\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "\n",
    "# Install taming-transformers (use rom1504 fork - this is what SyncDreamer requires!)\n",
    "!pip install -q taming-transformers-rom1504\n",
    "\n",
    "# Install image processing libraries\n",
    "!pip install -q rembg[gpu] opencv-python-headless scikit-image imageio\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Verify taming is installed correctly\n",
    "try:\n",
    "    from taming.modules.vqvae.quantize import VectorQuantizer2\n",
    "    print(\"‚úÖ taming-transformers installed correctly!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå taming import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1bac91",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download Pretrained Checkpoints\n",
    "\n",
    "Downloads two files (~6GB total):\n",
    "- `syncdreamer-pretrain.ckpt` (~5.2GB) - Main model\n",
    "- `ViT-L-14.pt` (~890MB) - CLIP encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/SyncDreamer\n",
    "\n",
    "# Install aria2 for faster downloads\n",
    "!apt -y install -qq aria2\n",
    "\n",
    "# Download SyncDreamer checkpoint from HuggingFace (~5.2GB)\n",
    "print(\"üì• Downloading SyncDreamer checkpoint (this may take 5-10 minutes)...\")\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \\\n",
    "    https://huggingface.co/camenduru/SyncDreamer/resolve/main/syncdreamer-pretrain.ckpt \\\n",
    "    -d /content/SyncDreamer/ckpt -o syncdreamer-pretrain.ckpt\n",
    "\n",
    "# Download CLIP ViT-L-14 encoder (~890MB)\n",
    "print(\"üì• Downloading CLIP ViT-L-14 encoder...\")\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \\\n",
    "    https://huggingface.co/camenduru/SyncDreamer/resolve/main/ViT-L-14.pt \\\n",
    "    -d /content/SyncDreamer/ckpt -o ViT-L-14.pt\n",
    "\n",
    "# Verify downloads\n",
    "import os\n",
    "ckpt_dir = \"/content/SyncDreamer/ckpt\"\n",
    "for f in [\"syncdreamer-pretrain.ckpt\", \"ViT-L-14.pt\"]:\n",
    "    path = os.path.join(ckpt_dir, f)\n",
    "    if os.path.exists(path):\n",
    "        size_gb = os.path.getsize(path) / (1024**3)\n",
    "        print(f\"‚úÖ {f}: {size_gb:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"‚ùå {f}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e0aef",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configure GPU Memory Settings for T4\n",
    "\n",
    "Optimized settings for T4 GPU (15GB VRAM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set environment variables for memory optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Enable memory-efficient settings\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.1f}GB total\")\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ GPU memory cache cleared\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "# Initial memory check\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bac79a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load SyncDreamer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e456313",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/SyncDreamer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/SyncDreamer')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "\n",
    "from ldm.models.diffusion.sync_dreamer import SyncMultiviewDiffusion, SyncDDIMSampler\n",
    "from ldm.util import instantiate_from_config, prepare_inputs\n",
    "\n",
    "def load_model(cfg_path, ckpt_path, device='cuda'):\n",
    "    \"\"\"Load SyncDreamer model\"\"\"\n",
    "    print(f\"üìÇ Loading config from {cfg_path}\")\n",
    "    config = OmegaConf.load(cfg_path)\n",
    "    \n",
    "    print(f\"üìÇ Loading checkpoint from {ckpt_path}\")\n",
    "    model = instantiate_from_config(config.model)\n",
    "    \n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['state_dict'], strict=True)\n",
    "    \n",
    "    model = model.to(device).eval()\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "CONFIG_PATH = \"/content/SyncDreamer/configs/syncdreamer.yaml\"\n",
    "CHECKPOINT_PATH = \"/content/SyncDreamer/ckpt/syncdreamer-pretrain.ckpt\"\n",
    "\n",
    "model = load_model(CONFIG_PATH, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fe529",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Download and Prepare Test Image\n",
    "\n",
    "**Recommended test images:**\n",
    "- Simple 3D objects (toys, furniture, shoes)\n",
    "- Clean backgrounds or objects that can be easily segmented\n",
    "- Front-facing view with ~30¬∞ elevation works best\n",
    "\n",
    "We'll use a sample image from the SyncDreamer test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Option 1: Use built-in test image from SyncDreamer\n",
    "TEST_IMAGE_PATH = \"/content/SyncDreamer/testset/aircraft.png\"\n",
    "\n",
    "# Option 2: Download a sample image (uncomment to use)\n",
    "# Sample images that work well with SyncDreamer:\n",
    "\n",
    "# Lysol bottle (commonly used for testing)\n",
    "# !wget -q https://huggingface.co/spaces/One-2-3-45/One-2-3-45/resolve/main/demo_examples/00_zero123_lysol.png -O /content/test_image.png\n",
    "# TEST_IMAGE_PATH = \"/content/test_image.png\"\n",
    "\n",
    "# Astronaut toy\n",
    "# !wget -q https://huggingface.co/spaces/One-2-3-45/One-2-3-45/resolve/main/demo_examples/01_astronaut.png -O /content/test_image.png\n",
    "# TEST_IMAGE_PATH = \"/content/test_image.png\"\n",
    "\n",
    "# Option 3: Upload your own image (uncomment to use)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# TEST_IMAGE_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# Display the test image\n",
    "print(f\"üì∑ Using test image: {TEST_IMAGE_PATH}\")\n",
    "img = Image.open(TEST_IMAGE_PATH)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Input Image ({img.size[0]}x{img.size[1]}, {img.mode})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {img.size}\")\n",
    "print(f\"Image mode: {img.mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af893a",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Preprocess Input Image\n",
    "\n",
    "For images without transparent backgrounds, we'll use rembg for background removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rembg import remove\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path, output_path=None, use_rembg=True):\n",
    "    \"\"\"\n",
    "    Preprocess image for SyncDreamer:\n",
    "    1. Remove background (if needed)\n",
    "    2. Convert to RGBA with transparent background\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Check if image already has alpha channel (transparent background)\n",
    "    if img.mode == 'RGBA':\n",
    "        alpha = np.array(img)[:, :, 3]\n",
    "        has_transparency = np.any(alpha < 255)\n",
    "        if has_transparency:\n",
    "            print(\"‚úÖ Image already has transparent background\")\n",
    "            if output_path:\n",
    "                img.save(output_path)\n",
    "            return img if not output_path else output_path\n",
    "    \n",
    "    # Remove background using rembg\n",
    "    if use_rembg:\n",
    "        print(\"üîÑ Removing background with rembg...\")\n",
    "        img_rgba = remove(img)\n",
    "        print(\"‚úÖ Background removed!\")\n",
    "    else:\n",
    "        img_rgba = img.convert('RGBA')\n",
    "    \n",
    "    if output_path:\n",
    "        img_rgba.save(output_path)\n",
    "        return output_path\n",
    "    return img_rgba\n",
    "\n",
    "# Preprocess the test image\n",
    "PROCESSED_IMAGE_PATH = \"/content/processed_input.png\"\n",
    "preprocess_image(TEST_IMAGE_PATH, PROCESSED_IMAGE_PATH, use_rembg=True)\n",
    "\n",
    "# Display processed image\n",
    "processed_img = Image.open(PROCESSED_IMAGE_PATH)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(Image.open(TEST_IMAGE_PATH))\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Processed (with alpha)\n",
    "# Show on checkered background to visualize transparency\n",
    "axes[1].imshow(processed_img)\n",
    "axes[1].set_title(\"Processed (Background Removed)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881a3c2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Run Multi-view Generation Inference\n",
    "\n",
    "**T4-optimized settings:**\n",
    "- `batch_view_num=4` (instead of 8) - processes 4 views at a time to reduce VRAM\n",
    "- `sample_num=1` - generates 1 set of 16 views\n",
    "- `sample_steps=50` - standard DDIM steps\n",
    "- `cfg_scale=2.0` - classifier-free guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from skimage.io import imsave\n",
    "\n",
    "# ============================================\n",
    "# INFERENCE PARAMETERS (Optimized for T4 GPU)\n",
    "# ============================================\n",
    "INPUT_IMAGE = PROCESSED_IMAGE_PATH\n",
    "OUTPUT_DIR = \"/content/output\"\n",
    "ELEVATION = 30.0        # Input view elevation (degrees) - adjust if needed\n",
    "CROP_SIZE = 200         # Foreground crop size (-1 to disable)\n",
    "CFG_SCALE = 2.0         # Classifier-free guidance scale\n",
    "BATCH_VIEW_NUM = 4      # ‚ö†Ô∏è KEY SETTING: 4 for T4 (15GB), 8 for A100\n",
    "SAMPLE_NUM = 1          # Number of sample sets to generate\n",
    "SAMPLE_STEPS = 50       # DDIM sampling steps\n",
    "SEED = 42               # Random seed for reproducibility\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üöÄ Running SyncDreamer Inference\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üì∑ Input: {INPUT_IMAGE}\")\n",
    "print(f\"üìê Elevation: {ELEVATION}¬∞\")\n",
    "print(f\"üéØ CFG Scale: {CFG_SCALE}\")\n",
    "print(f\"üì¶ Batch View Num: {BATCH_VIEW_NUM}\")\n",
    "print(f\"üî¢ Sample Steps: {SAMPLE_STEPS}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare input data\n",
    "print(\"\\nüìä Preparing input data...\")\n",
    "data = prepare_inputs(INPUT_IMAGE, ELEVATION, CROP_SIZE)\n",
    "for k, v in data.items():\n",
    "    data[k] = v.unsqueeze(0).cuda()\n",
    "    data[k] = torch.repeat_interleave(data[k], SAMPLE_NUM, dim=0)\n",
    "\n",
    "print_gpu_memory()\n",
    "\n",
    "# Create sampler\n",
    "print(\"\\nüé≤ Creating DDIM sampler...\")\n",
    "sampler = SyncDDIMSampler(model, SAMPLE_STEPS)\n",
    "\n",
    "# Run inference\n",
    "print(\"\\n‚è≥ Generating 16 multi-view images...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_sample = model.sample(sampler, data, CFG_SCALE, BATCH_VIEW_NUM)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Generation complete in {elapsed_time:.1f} seconds!\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# Process output\n",
    "B, N, C, H, W = x_sample.shape\n",
    "print(f\"üìê Output shape: {x_sample.shape} (B={B}, N={N} views, {H}x{W})\")\n",
    "\n",
    "x_sample = (torch.clamp(x_sample, max=1.0, min=-1.0) + 1) * 0.5\n",
    "x_sample = x_sample.permute(0, 1, 3, 4, 2).cpu().numpy() * 255\n",
    "x_sample = x_sample.astype(np.uint8)\n",
    "\n",
    "# Store for visualization\n",
    "generated_views = x_sample[0]  # First batch item, shape: (16, H, W, 3)\n",
    "print(f\"‚úÖ Generated {generated_views.shape[0]} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f6b89",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Visualize Generated Views\n",
    "\n",
    "The 16 views are arranged as:\n",
    "- **Row 1-2**: Elevation 30¬∞ (views 0-7)\n",
    "- **Row 3-4**: Elevation -20¬∞ (views 8-15)\n",
    "- **Columns**: Azimuths 0¬∞, 45¬∞, 90¬∞, 135¬∞, 180¬∞, 225¬∞, 270¬∞, 315¬∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4x4 grid visualization\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "elevations = [30, 30, 30, 30, 30, 30, 30, 30, -20, -20, -20, -20, -20, -20, -20, -20]\n",
    "azimuths = [0, 45, 90, 135, 180, 225, 270, 315, 0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "for i in range(16):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    axes[row, col].imshow(generated_views[i])\n",
    "    axes[row, col].set_title(f\"View {i}: E={elevations[i]}¬∞ A={azimuths[i]}¬∞\", fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"SyncDreamer Generated Multi-View Images\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/multiview_grid.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Grid saved to {OUTPUT_DIR}/multiview_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7697d0",
   "metadata": {},
   "source": [
    "## üîü Save Output Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# Save individual views\n",
    "print(\"üíæ Saving individual views...\")\n",
    "saved_paths = []\n",
    "for i in range(16):\n",
    "    filename = f\"view_{i:02d}_elev{elevations[i]}_azim{azimuths[i]}.png\"\n",
    "    path = os.path.join(OUTPUT_DIR, filename)\n",
    "    imsave(path, generated_views[i])\n",
    "    saved_paths.append(path)\n",
    "    \n",
    "print(f\"‚úÖ Saved {len(saved_paths)} individual views to {OUTPUT_DIR}/\")\n",
    "\n",
    "# Save concatenated strip (original SyncDreamer format)\n",
    "concat_image = np.concatenate([generated_views[i] for i in range(16)], axis=1)\n",
    "imsave(f\"{OUTPUT_DIR}/concat_strip.png\", concat_image)\n",
    "print(f\"‚úÖ Saved concatenated strip to {OUTPUT_DIR}/concat_strip.png\")\n",
    "\n",
    "# Create animated GIF (turntable rotation)\n",
    "print(\"üé¨ Creating turntable animation...\")\n",
    "\n",
    "# Use first 8 views (elevation 30¬∞) for turntable\n",
    "turntable_views = [generated_views[i] for i in range(8)]\n",
    "# Add reverse for smooth loop\n",
    "turntable_views_loop = turntable_views + turntable_views[::-1][1:-1]\n",
    "\n",
    "gif_path = f\"{OUTPUT_DIR}/turntable.gif\"\n",
    "imageio.mimsave(gif_path, turntable_views_loop, fps=4, loop=0)\n",
    "print(f\"‚úÖ Saved turntable GIF to {gif_path}\")\n",
    "\n",
    "# Display the GIF\n",
    "from IPython.display import Image as IPImage, display\n",
    "display(IPImage(filename=gif_path))\n",
    "\n",
    "# List all output files\n",
    "print(\"\\nüìÅ Output files:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    size_kb = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1024\n",
    "    print(f\"  - {f} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c056a",
   "metadata": {},
   "source": [
    "## üì• Download Results (Optional)\n",
    "\n",
    "Run this cell to download all outputs as a ZIP file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421096e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create ZIP archive\n",
    "zip_path = \"/content/syncdreamer_output\"\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "print(f\"üì¶ Created {zip_path}.zip\")\n",
    "\n",
    "# Download\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825550fe",
   "metadata": {},
   "source": [
    "## üßπ Cleanup (Free GPU Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518523df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "del sampler\n",
    "del x_sample\n",
    "clear_gpu_memory()\n",
    "\n",
    "print(\"‚úÖ GPU memory freed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d184ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes & Tips\n",
    "\n",
    "### T4 GPU Memory Optimization\n",
    "- `batch_view_num=4` is optimal for T4 (15GB). Use `8` for A100/V100.\n",
    "- If you get OOM errors, try `batch_view_num=2`\n",
    "\n",
    "### Best Input Images\n",
    "- **Size**: Any size (will be resized to 256x256)\n",
    "- **Background**: Transparent (RGBA) works best\n",
    "- **Subject**: Centered, single object\n",
    "- **Elevation**: ~30¬∞ from front works best\n",
    "\n",
    "### Elevation Tips\n",
    "- Front-facing photos: `elevation=30`\n",
    "- Top-down photos: `elevation=60-80`\n",
    "- Eye-level photos: `elevation=0-20`\n",
    "\n",
    "### Recommended Test Images\n",
    "1. **Aircraft** (built-in): `/content/SyncDreamer/testset/aircraft.png`\n",
    "2. **Lysol bottle**: `https://huggingface.co/spaces/One-2-3-45/One-2-3-45/resolve/main/demo_examples/00_zero123_lysol.png`\n",
    "3. **Astronaut**: `https://huggingface.co/spaces/One-2-3-45/One-2-3-45/resolve/main/demo_examples/01_astronaut.png`\n",
    "\n",
    "### Citation\n",
    "```bibtex\n",
    "@article{liu2023syncdreamer,\n",
    "  title={SyncDreamer: Generating Multiview-consistent Images from a Single-view Image},\n",
    "  author={Liu, Yuan and Lin, Cheng and Zeng, Zijiao and Long, Xiaoxiao and Liu, Lingjie and Komura, Taku and Wang, Wenping},\n",
    "  journal={arXiv preprint arXiv:2309.03453},\n",
    "  year={2023}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
