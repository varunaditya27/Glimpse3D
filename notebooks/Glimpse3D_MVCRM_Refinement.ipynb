{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5b3bc1",
   "metadata": {},
   "source": [
    "# üîÑ Glimpse3D - MVCRM Multi-View Consistent Refinement\n",
    "\n",
    "**Enhance 3D representations using diffusion-enhanced multi-view images**\n",
    "\n",
    "This notebook implements the **Multi-View Consistent Refinement Module (MVCRM)**, which:\n",
    "1. Enhances rendered views with SDXL Lightning + ControlNet\n",
    "2. Back-projects enhanced pixels into 3D space\n",
    "3. Updates Gaussian splat parameters for consistent refinement\n",
    "\n",
    "## Pipeline Role\n",
    "```\n",
    "TripoSR ‚Üí gsplat ‚Üí SyncDreamer ‚Üí SDXL Enhancement ‚Üí [This Notebook] ‚Üí Final Output\n",
    "```\n",
    "\n",
    "## Novel Contribution\n",
    "This is a **key innovation** in the Glimpse3D pipeline - ensuring 2D diffusion enhancements are propagated back consistently into 3D.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32550697",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22976eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Colab: {IN_COLAB}\")\n",
    "\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09182643",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Core dependencies\n",
    "!pip install torch torchvision --quiet\n",
    "!pip install diffusers transformers accelerate --quiet\n",
    "!pip install gsplat plyfile --quiet\n",
    "!pip install numpy pillow matplotlib tqdm opencv-python-headless --quiet\n",
    "!pip install scipy scikit-image --quiet\n",
    "\n",
    "# For depth estimation\n",
    "!pip install timm --quiet\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376abb2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Setup Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c78bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "WORK_DIR = Path(\"/content/mvcrm_work\")\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Subdirectories\n",
    "(WORK_DIR / \"renders\").mkdir(exist_ok=True)\n",
    "(WORK_DIR / \"enhanced\").mkdir(exist_ok=True)\n",
    "(WORK_DIR / \"output\").mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f6a62",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Upload Inputs\n",
    "\n",
    "Required inputs:\n",
    "- Optimized Gaussian PLY from gsplat\n",
    "- Multi-view images from SyncDreamer OR enhanced images from SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8250dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Upload Gaussian PLY\n",
    "print(\"üì§ Upload optimized Gaussian PLY:\")\n",
    "uploaded_ply = files.upload()\n",
    "PLY_PATH = WORK_DIR / list(uploaded_ply.keys())[0]\n",
    "with open(PLY_PATH, 'wb') as f:\n",
    "    f.write(list(uploaded_ply.values())[0])\n",
    "print(f\"‚úÖ PLY saved: {PLY_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload enhanced images (ZIP or individual)\n",
    "print(\"\\nüì§ Upload enhanced images (ZIP or individual PNGs):\")\n",
    "uploaded_images = files.upload()\n",
    "\n",
    "IMAGE_DIR = WORK_DIR / \"enhanced\"\n",
    "\n",
    "for fname, content in uploaded_images.items():\n",
    "    if fname.endswith('.zip'):\n",
    "        zip_path = WORK_DIR / fname\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(IMAGE_DIR)\n",
    "        print(f\"‚úÖ Extracted: {fname}\")\n",
    "    else:\n",
    "        img_path = IMAGE_DIR / fname\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\"‚úÖ Saved: {fname}\")\n",
    "\n",
    "# List images\n",
    "enhanced_images = sorted([f for f in IMAGE_DIR.iterdir() if f.suffix.lower() in ['.png', '.jpg', '.jpeg']])\n",
    "print(f\"\\n‚úÖ Found {len(enhanced_images)} enhanced images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327fc80",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load Gaussian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_gaussian_ply(path):\n",
    "    \"\"\"Load Gaussian Splat PLY file into tensors.\"\"\"\n",
    "    plydata = PlyData.read(path)\n",
    "    vertex = plydata['vertex']\n",
    "    \n",
    "    xyz = np.stack([vertex['x'], vertex['y'], vertex['z']], axis=-1)\n",
    "    f_dc = np.stack([vertex['f_dc_0'], vertex['f_dc_1'], vertex['f_dc_2']], axis=-1)\n",
    "    \n",
    "    f_rest_names = [f'f_rest_{i}' for i in range(45)]\n",
    "    f_rest = np.stack([vertex[name] for name in f_rest_names if name in vertex.data.dtype.names], axis=-1)\n",
    "    \n",
    "    opacity = vertex['opacity']\n",
    "    scales = np.stack([vertex['scale_0'], vertex['scale_1'], vertex['scale_2']], axis=-1)\n",
    "    rotations = np.stack([vertex['rot_0'], vertex['rot_1'], vertex['rot_2'], vertex['rot_3']], axis=-1)\n",
    "    \n",
    "    return {\n",
    "        'xyz': torch.tensor(xyz, dtype=torch.float32, device=device),\n",
    "        'f_dc': torch.tensor(f_dc, dtype=torch.float32, device=device),\n",
    "        'f_rest': torch.tensor(f_rest, dtype=torch.float32, device=device),\n",
    "        'opacity': torch.tensor(opacity, dtype=torch.float32, device=device),\n",
    "        'scales': torch.tensor(scales, dtype=torch.float32, device=device),\n",
    "        'rotations': torch.tensor(rotations, dtype=torch.float32, device=device),\n",
    "    }\n",
    "\n",
    "class GaussianModel(nn.Module):\n",
    "    def __init__(self, gaussians):\n",
    "        super().__init__()\n",
    "        self.xyz = nn.Parameter(gaussians['xyz'].clone())\n",
    "        self.f_dc = nn.Parameter(gaussians['f_dc'].clone())\n",
    "        self.f_rest = nn.Parameter(gaussians['f_rest'].clone())\n",
    "        self.opacity_raw = nn.Parameter(gaussians['opacity'].clone())\n",
    "        self.scales_raw = nn.Parameter(gaussians['scales'].clone())\n",
    "        self.rotations = nn.Parameter(gaussians['rotations'].clone())\n",
    "        \n",
    "    @property\n",
    "    def opacity(self):\n",
    "        return torch.sigmoid(self.opacity_raw)\n",
    "    \n",
    "    @property\n",
    "    def scales(self):\n",
    "        return torch.exp(self.scales_raw)\n",
    "    \n",
    "    def get_colors(self):\n",
    "        C0 = 0.28209479177387814\n",
    "        return 0.5 + C0 * self.f_dc\n",
    "    \n",
    "    def forward(self):\n",
    "        return {\n",
    "            'xyz': self.xyz,\n",
    "            'colors': self.get_colors(),\n",
    "            'opacity': self.opacity,\n",
    "            'scales': self.scales,\n",
    "            'rotations': self.rotations / (self.rotations.norm(dim=-1, keepdim=True) + 1e-8),\n",
    "        }\n",
    "\n",
    "# Load model\n",
    "gaussians = load_gaussian_ply(PLY_PATH)\n",
    "model = GaussianModel(gaussians).to(device)\n",
    "print(f\"‚úÖ Loaded {len(gaussians['xyz']):,} Gaussians\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf18141",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Camera System for SyncDreamer Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b580a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# SyncDreamer camera configuration (16 views)\n",
    "# Views 0-7: Elevation 30¬∞, Azimuth 0¬∞, 45¬∞, 90¬∞, ..., 315¬∞\n",
    "# Views 8-15: Elevation -20¬∞, Azimuth 0¬∞, 45¬∞, 90¬∞, ..., 315¬∞\n",
    "\n",
    "SYNCDREAMER_ELEVATIONS = [30.0] * 8 + [-20.0] * 8\n",
    "SYNCDREAMER_AZIMUTHS = [i * 45.0 for i in range(8)] * 2\n",
    "\n",
    "def create_camera_pose(elevation_deg, azimuth_deg, radius=2.0):\n",
    "    \"\"\"Create world-to-camera matrix for given elevation and azimuth.\"\"\"\n",
    "    elev = math.radians(elevation_deg)\n",
    "    azim = math.radians(azimuth_deg)\n",
    "    \n",
    "    # Camera position\n",
    "    x = radius * math.cos(elev) * math.cos(azim)\n",
    "    y = radius * math.cos(elev) * math.sin(azim)\n",
    "    z = radius * math.sin(elev)\n",
    "    \n",
    "    cam_pos = np.array([x, y, z])\n",
    "    look_at = np.array([0, 0, 0])\n",
    "    up = np.array([0, 0, 1])\n",
    "    \n",
    "    forward = look_at - cam_pos\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    up_new = np.cross(right, forward)\n",
    "    \n",
    "    w2c = np.eye(4)\n",
    "    w2c[:3, 0] = right\n",
    "    w2c[:3, 1] = up_new\n",
    "    w2c[:3, 2] = -forward\n",
    "    w2c[:3, 3] = -w2c[:3, :3] @ cam_pos\n",
    "    \n",
    "    return w2c\n",
    "\n",
    "def get_projection_matrix(fov_deg=60, aspect=1.0, near=0.1, far=100.0):\n",
    "    fov_rad = math.radians(fov_deg)\n",
    "    f = 1.0 / math.tan(fov_rad / 2)\n",
    "    \n",
    "    proj = np.zeros((4, 4))\n",
    "    proj[0, 0] = f / aspect\n",
    "    proj[1, 1] = f\n",
    "    proj[2, 2] = (far + near) / (near - far)\n",
    "    proj[2, 3] = 2 * far * near / (near - far)\n",
    "    proj[3, 2] = -1\n",
    "    \n",
    "    return proj\n",
    "\n",
    "# Generate all camera poses\n",
    "camera_poses = [create_camera_pose(e, a) for e, a in zip(SYNCDREAMER_ELEVATIONS, SYNCDREAMER_AZIMUTHS)]\n",
    "projection = get_projection_matrix(fov_deg=60)\n",
    "\n",
    "print(f\"‚úÖ Created {len(camera_poses)} camera poses matching SyncDreamer configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39462d8b",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Render and Compute Pixel-to-Gaussian Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd072846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsplat import rasterization\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_SIZE = 512\n",
    "\n",
    "def render_with_info(model, w2c, proj, image_size=512):\n",
    "    \"\"\"\n",
    "    Render Gaussians and return:\n",
    "    - RGB image\n",
    "    - Alpha mask\n",
    "    - Per-pixel Gaussian indices (for back-projection)\n",
    "    \"\"\"\n",
    "    params = model()\n",
    "    \n",
    "    viewmat = torch.tensor(w2c, dtype=torch.float32, device=device)\n",
    "    K = torch.tensor([\n",
    "        [proj[0, 0] * image_size / 2, 0, image_size / 2],\n",
    "        [0, proj[1, 1] * image_size / 2, image_size / 2],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=torch.float32, device=device)\n",
    "    \n",
    "    render_colors, render_alphas, meta = rasterization(\n",
    "        means=params['xyz'],\n",
    "        quats=params['rotations'],\n",
    "        scales=params['scales'],\n",
    "        opacities=params['opacity'],\n",
    "        colors=params['colors'],\n",
    "        viewmats=viewmat.unsqueeze(0),\n",
    "        Ks=K.unsqueeze(0),\n",
    "        width=image_size,\n",
    "        height=image_size,\n",
    "        packed=False,\n",
    "        render_mode=\"RGB\",\n",
    "    )\n",
    "    \n",
    "    return render_colors[0], render_alphas[0], meta\n",
    "\n",
    "# Render all views and save\n",
    "print(\"üé¨ Rendering all views...\")\n",
    "rendered_views = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (w2c, elev, azim) in enumerate(zip(camera_poses, SYNCDREAMER_ELEVATIONS, SYNCDREAMER_AZIMUTHS)):\n",
    "        rgb, alpha, _ = render_with_info(model, w2c, projection, IMAGE_SIZE)\n",
    "        \n",
    "        # Save render\n",
    "        img_np = (rgb.cpu().numpy().clip(0, 1) * 255).astype(np.uint8)\n",
    "        render_path = WORK_DIR / \"renders\" / f\"render_{i:02d}.png\"\n",
    "        Image.fromarray(img_np).save(render_path)\n",
    "        \n",
    "        rendered_views.append(rgb)\n",
    "        print(f\"  View {i}: E={elev}¬∞ A={azim}¬∞\")\n",
    "\n",
    "print(f\"\\n‚úÖ Rendered {len(rendered_views)} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec641e",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Load Enhanced Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a822cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load enhanced images\n",
    "enhanced_tensors = []\n",
    "\n",
    "for img_path in sorted(enhanced_images)[:16]:  # Use first 16 images\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_tensor = torch.tensor(np.array(img) / 255.0, dtype=torch.float32, device=device)\n",
    "    enhanced_tensors.append(img_tensor)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(enhanced_tensors)} enhanced images\")\n",
    "\n",
    "# Visualize comparison\n",
    "if len(enhanced_tensors) > 0 and len(rendered_views) > 0:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    for i in range(min(4, len(enhanced_tensors))):\n",
    "        axes[0, i].imshow(rendered_views[i].cpu().numpy().clip(0, 1))\n",
    "        axes[0, i].set_title(f\"Original Render {i}\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(enhanced_tensors[i].cpu().numpy().clip(0, 1))\n",
    "        axes[1, i].set_title(f\"Enhanced {i}\")\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Original vs Enhanced Views\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8b958",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Multi-View Consistent Refinement (MVCRM)\n",
    "\n",
    "The key innovation: back-project 2D enhancements into 3D while maintaining consistency across views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MVCRMRefiner:\n",
    "    \"\"\"Multi-View Consistent Refinement Module.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, camera_poses, projection, image_size=512):\n",
    "        self.model = model\n",
    "        self.camera_poses = camera_poses\n",
    "        self.projection = projection\n",
    "        self.image_size = image_size\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "    def compute_color_loss(self, rendered, target):\n",
    "        \"\"\"L1 + Perceptual loss between rendered and target.\"\"\"\n",
    "        l1_loss = F.l1_loss(rendered, target)\n",
    "        \n",
    "        # Simple luminance-based perceptual loss\n",
    "        render_lum = 0.299 * rendered[..., 0] + 0.587 * rendered[..., 1] + 0.114 * rendered[..., 2]\n",
    "        target_lum = 0.299 * target[..., 0] + 0.587 * target[..., 1] + 0.114 * target[..., 2]\n",
    "        \n",
    "        # Sobel-like edge detection\n",
    "        render_grad_x = render_lum[:, 1:] - render_lum[:, :-1]\n",
    "        render_grad_y = render_lum[1:, :] - render_lum[:-1, :]\n",
    "        target_grad_x = target_lum[:, 1:] - target_lum[:, :-1]\n",
    "        target_grad_y = target_lum[1:, :] - target_lum[:-1, :]\n",
    "        \n",
    "        edge_loss = F.l1_loss(render_grad_x, target_grad_x) + F.l1_loss(render_grad_y, target_grad_y)\n",
    "        \n",
    "        return l1_loss + 0.1 * edge_loss\n",
    "    \n",
    "    def refine(self, enhanced_images, num_iterations=500, lr_color=1e-3, lr_position=1e-5):\n",
    "        \"\"\"Refine Gaussians to match enhanced images.\"\"\"\n",
    "        \n",
    "        # Setup optimizer - only optimize colors and positions\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': self.model.f_dc, 'lr': lr_color},\n",
    "            {'params': self.model.xyz, 'lr': lr_position},\n",
    "        ])\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_iterations)\n",
    "        \n",
    "        losses = []\n",
    "        n_views = len(enhanced_images)\n",
    "        \n",
    "        print(f\"üîÑ Starting MVCRM refinement with {n_views} views...\")\n",
    "        pbar = tqdm(range(num_iterations))\n",
    "        \n",
    "        for iteration in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "            \n",
    "            # Sample a subset of views for this iteration\n",
    "            view_indices = np.random.choice(n_views, size=min(4, n_views), replace=False)\n",
    "            \n",
    "            for view_idx in view_indices:\n",
    "                w2c = self.camera_poses[view_idx]\n",
    "                target = enhanced_images[view_idx]\n",
    "                \n",
    "                # Render\n",
    "                rendered, alpha, _ = render_with_info(self.model, w2c, self.projection, self.image_size)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.compute_color_loss(rendered, target)\n",
    "                total_loss += loss\n",
    "            \n",
    "            avg_loss = total_loss / len(view_indices)\n",
    "            avg_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            losses.append(avg_loss.item())\n",
    "            \n",
    "            if iteration % 50 == 0:\n",
    "                pbar.set_postfix({'loss': f'{avg_loss.item():.4f}'})\n",
    "        \n",
    "        print(f\"\\n‚úÖ Refinement complete! Final loss: {losses[-1]:.4f}\")\n",
    "        return losses\n",
    "\n",
    "# Initialize refiner\n",
    "refiner = MVCRMRefiner(model, camera_poses, projection, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb98b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run refinement\n",
    "if len(enhanced_tensors) > 0:\n",
    "    losses = refiner.refine(\n",
    "        enhanced_tensors,\n",
    "        num_iterations=500,\n",
    "        lr_color=1e-3,\n",
    "        lr_position=1e-5\n",
    "    )\n",
    "    \n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('MVCRM Refinement Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No enhanced images loaded. Skipping refinement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32b13e",
   "metadata": {},
   "source": [
    "## üîü Compare Before/After Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render refined views\n",
    "print(\"üé¨ Rendering refined views...\")\n",
    "refined_views = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, w2c in enumerate(camera_poses):\n",
    "        rgb, alpha, _ = render_with_info(model, w2c, projection, IMAGE_SIZE)\n",
    "        refined_views.append(rgb)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(4):\n",
    "    # Original render\n",
    "    axes[0, i].imshow(rendered_views[i].cpu().numpy().clip(0, 1))\n",
    "    axes[0, i].set_title(f\"Original {i}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Enhanced target\n",
    "    if i < len(enhanced_tensors):\n",
    "        axes[1, i].imshow(enhanced_tensors[i].cpu().numpy().clip(0, 1))\n",
    "    axes[1, i].set_title(f\"Enhanced Target {i}\")\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Refined render\n",
    "    axes[2, i].imshow(refined_views[i].cpu().numpy().clip(0, 1))\n",
    "    axes[2, i].set_title(f\"Refined {i}\")\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"MVCRM: Original ‚Üí Enhanced Target ‚Üí Refined Result\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(WORK_DIR / \"output\" / \"comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adcec4d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Export Refined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyElement, PlyData\n",
    "\n",
    "def save_gaussian_ply(model, output_path):\n",
    "    \"\"\"Save Gaussian model to PLY file.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        params = model()\n",
    "        \n",
    "        xyz = params['xyz'].cpu().numpy()\n",
    "        colors = model.f_dc.cpu().numpy()\n",
    "        f_rest = model.f_rest.cpu().numpy()\n",
    "        opacity = model.opacity_raw.cpu().numpy()\n",
    "        scales = model.scales_raw.cpu().numpy()\n",
    "        rotations = params['rotations'].cpu().numpy()\n",
    "        \n",
    "    num_points = len(xyz)\n",
    "    \n",
    "    dtype_full = [\n",
    "        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "        ('f_dc_0', 'f4'), ('f_dc_1', 'f4'), ('f_dc_2', 'f4'),\n",
    "    ]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        dtype_full.append((f'f_rest_{i}', 'f4'))\n",
    "    dtype_full.extend([\n",
    "        ('opacity', 'f4'),\n",
    "        ('scale_0', 'f4'), ('scale_1', 'f4'), ('scale_2', 'f4'),\n",
    "        ('rot_0', 'f4'), ('rot_1', 'f4'), ('rot_2', 'f4'), ('rot_3', 'f4'),\n",
    "    ])\n",
    "    \n",
    "    elements = np.zeros(num_points, dtype=dtype_full)\n",
    "    elements['x'] = xyz[:, 0]\n",
    "    elements['y'] = xyz[:, 1]\n",
    "    elements['z'] = xyz[:, 2]\n",
    "    elements['f_dc_0'] = colors[:, 0]\n",
    "    elements['f_dc_1'] = colors[:, 1]\n",
    "    elements['f_dc_2'] = colors[:, 2]\n",
    "    for i in range(f_rest.shape[1]):\n",
    "        elements[f'f_rest_{i}'] = f_rest[:, i]\n",
    "    elements['opacity'] = opacity\n",
    "    elements['scale_0'] = scales[:, 0]\n",
    "    elements['scale_1'] = scales[:, 1]\n",
    "    elements['scale_2'] = scales[:, 2]\n",
    "    elements['rot_0'] = rotations[:, 0]\n",
    "    elements['rot_1'] = rotations[:, 1]\n",
    "    elements['rot_2'] = rotations[:, 2]\n",
    "    elements['rot_3'] = rotations[:, 3]\n",
    "    \n",
    "    el = PlyElement.describe(elements, 'vertex')\n",
    "    PlyData([el]).write(output_path)\n",
    "    print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "# Save refined model\n",
    "refined_ply_path = WORK_DIR / \"output\" / \"refined_gaussian.ply\"\n",
    "save_gaussian_ply(model, str(refined_ply_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bd9b8",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Create Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45cb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# Generate smooth 360¬∞ rotation\n",
    "print(\"üé¨ Rendering 360¬∞ video...\")\n",
    "video_frames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for azim in tqdm(np.linspace(0, 360, 60)):\n",
    "        w2c = create_camera_pose(30.0, azim, radius=2.0)\n",
    "        rgb, _, _ = render_with_info(model, w2c, projection, IMAGE_SIZE)\n",
    "        frame = (rgb.cpu().numpy().clip(0, 1) * 255).astype(np.uint8)\n",
    "        video_frames.append(frame)\n",
    "\n",
    "# Save video\n",
    "video_path = WORK_DIR / \"output\" / \"refined_360.mp4\"\n",
    "imageio.mimsave(str(video_path), video_frames, fps=30)\n",
    "print(f\"‚úÖ Saved video: {video_path}\")\n",
    "\n",
    "# Display in notebook\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "mp4 = open(video_path, 'rb').read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "HTML(f'<video width=400 controls autoplay loop><source src=\"{data_url}\" type=\"video/mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effecbdb",
   "metadata": {},
   "source": [
    "## üì• Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create output ZIP\n",
    "output_zip = shutil.make_archive(\n",
    "    str(WORK_DIR / \"mvcrm_output\"),\n",
    "    'zip',\n",
    "    WORK_DIR / \"output\"\n",
    ")\n",
    "\n",
    "print(\"üì• Downloading results...\")\n",
    "files.download(output_zip)\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce9a0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ MVCRM Complete!\n",
    "\n",
    "The **Multi-View Consistent Refinement Module** has:\n",
    "1. Loaded the optimized Gaussian splats\n",
    "2. Aligned camera poses with SyncDreamer configuration\n",
    "3. Back-projected enhanced 2D images into 3D\n",
    "4. Refined Gaussian parameters for multi-view consistency\n",
    "\n",
    "The output `refined_gaussian.ply` is the final Glimpse3D result!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
